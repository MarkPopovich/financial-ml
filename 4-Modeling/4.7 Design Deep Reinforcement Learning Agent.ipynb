{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "4.7 Design Deep Reinforcement Learning Agent.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm6b5yaR6OH-",
        "colab_type": "text"
      },
      "source": [
        "# 4.7 Design Deep Reinforcement Learning Agent\n",
        "\n",
        "In this notebook we will consider an alternative question. Instead of asking how we can maximize profit on a single asset, we will ask whether a machine can pick the best assets given many selections. \n",
        "\n",
        "We will consider only price history data. We will provide 90 differenced timesteps on the minute interval period. We will choose the 500 stocks of the S&P 500. \n",
        "\n",
        "If our network can outperform the S&P 500 over the given time, we will consider it successful. \n",
        "\n",
        "In order to do this, we will need to perform the following steps:\n",
        "\n",
        "- download datasets for all the stocks in the S&P 500. \n",
        "- format the data to represent the simulataneous movement of 500 stocks\n",
        "- Build an environment to represent this movement\n",
        "- Train a DQN to learn on it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1kAIK7f6OIA",
        "colab_type": "text"
      },
      "source": [
        "#### Download datasets for all the stocks in the S&P 500."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1xW-_jbiS1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython import display\n",
        "!pip install gcsfs\n",
        "!pip install tf-agents[reverb]\n",
        "display.clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ_5JnXK1vQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "import time\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import base64\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "\n",
        "from tf_agents.agents.ddpg import critic_network\n",
        "from tf_agents.agents.sac import sac_agent\n",
        "from tf_agents.agents.sac import tanh_normal_projection_network\n",
        "from tf_agents.experimental.train import actor\n",
        "from tf_agents.experimental.train import learner\n",
        "from tf_agents.experimental.train import triggers\n",
        "from tf_agents.experimental.train.utils import spec_utils\n",
        "from tf_agents.experimental.train.utils import strategy_utils\n",
        "from tf_agents.experimental.train.utils import train_utils\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "import reverb\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs_lLrRTasCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drive.mount('/content/drive')\n",
        "\n",
        "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/content/drive/My Drive/fin-aml/fin-aml-gcskeys-337dfd8d8867.json\"\n",
        "\n",
        "# storage_client = storage.Client()\n",
        "# display.clear_output()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GhPWMKn1vQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "tempdir = tempfile.gettempdir()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQTvbiXI65JW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df = pd.read_csv('gs://fin-datasets-aml/data/sp500/spdfm.csv')\n",
        "# df.set_index('datetime', inplace=True)\n",
        "# df.head()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVPJeAq3Fv59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "be2a43ae-b21e-4a1e-cb47-c6ed7d369372"
      },
      "source": [
        "# create toy data to test whether the model works\n",
        "# 505 feature space X\n",
        "# each X_i = A_i * x + B_i where x exists in {-100, 100}\n",
        "# a simple line from -100 to 100 to test to the model\n",
        "import numpy as np\n",
        "A = np.random.normal(0, .1, size=505).reshape(1,505)\n",
        "B = np.random.normal(0, .1, size=505).reshape(1,505)\n",
        "x = np.linspace(-1, 1, 2000).reshape(2000,1)\n",
        "\n",
        "X = np.matmul(x, A) + B\n",
        "X.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 505)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpjoSIIf6OIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X = df[::-1].to_numpy()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r5R-OBwMw3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# mx = MinMaxScaler([-.5,.5])\n",
        "# X = mx.fit_transform(X)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QrwFBPi6OIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b4e3fdee-9526-4e44-bfaf-df1de77821ed"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 505)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK98aSZKN_Ss",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "d53f9fbd-442a-48d1-daad-1914a5896615"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "pd.DataFrame(X)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>...</th>\n",
              "      <th>455</th>\n",
              "      <th>456</th>\n",
              "      <th>457</th>\n",
              "      <th>458</th>\n",
              "      <th>459</th>\n",
              "      <th>460</th>\n",
              "      <th>461</th>\n",
              "      <th>462</th>\n",
              "      <th>463</th>\n",
              "      <th>464</th>\n",
              "      <th>465</th>\n",
              "      <th>466</th>\n",
              "      <th>467</th>\n",
              "      <th>468</th>\n",
              "      <th>469</th>\n",
              "      <th>470</th>\n",
              "      <th>471</th>\n",
              "      <th>472</th>\n",
              "      <th>473</th>\n",
              "      <th>474</th>\n",
              "      <th>475</th>\n",
              "      <th>476</th>\n",
              "      <th>477</th>\n",
              "      <th>478</th>\n",
              "      <th>479</th>\n",
              "      <th>480</th>\n",
              "      <th>481</th>\n",
              "      <th>482</th>\n",
              "      <th>483</th>\n",
              "      <th>484</th>\n",
              "      <th>485</th>\n",
              "      <th>486</th>\n",
              "      <th>487</th>\n",
              "      <th>488</th>\n",
              "      <th>489</th>\n",
              "      <th>490</th>\n",
              "      <th>491</th>\n",
              "      <th>492</th>\n",
              "      <th>493</th>\n",
              "      <th>494</th>\n",
              "      <th>495</th>\n",
              "      <th>496</th>\n",
              "      <th>497</th>\n",
              "      <th>498</th>\n",
              "      <th>499</th>\n",
              "      <th>500</th>\n",
              "      <th>501</th>\n",
              "      <th>502</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.164540</td>\n",
              "      <td>-0.043503</td>\n",
              "      <td>-0.084499</td>\n",
              "      <td>0.152406</td>\n",
              "      <td>0.285525</td>\n",
              "      <td>-0.056006</td>\n",
              "      <td>-0.150748</td>\n",
              "      <td>-0.010962</td>\n",
              "      <td>0.013536</td>\n",
              "      <td>0.154553</td>\n",
              "      <td>-0.036267</td>\n",
              "      <td>0.086576</td>\n",
              "      <td>-0.000450</td>\n",
              "      <td>0.147094</td>\n",
              "      <td>0.243426</td>\n",
              "      <td>-0.020298</td>\n",
              "      <td>-0.083235</td>\n",
              "      <td>-0.115102</td>\n",
              "      <td>0.111463</td>\n",
              "      <td>-0.049863</td>\n",
              "      <td>-0.040667</td>\n",
              "      <td>0.077360</td>\n",
              "      <td>0.051466</td>\n",
              "      <td>-0.090031</td>\n",
              "      <td>-0.247560</td>\n",
              "      <td>-0.076548</td>\n",
              "      <td>-0.276460</td>\n",
              "      <td>0.130887</td>\n",
              "      <td>-0.033427</td>\n",
              "      <td>0.319263</td>\n",
              "      <td>0.197019</td>\n",
              "      <td>0.027940</td>\n",
              "      <td>0.070874</td>\n",
              "      <td>-0.217744</td>\n",
              "      <td>-0.049487</td>\n",
              "      <td>0.013633</td>\n",
              "      <td>-0.051024</td>\n",
              "      <td>0.342613</td>\n",
              "      <td>-0.061123</td>\n",
              "      <td>0.007975</td>\n",
              "      <td>0.094679</td>\n",
              "      <td>-0.060865</td>\n",
              "      <td>0.160515</td>\n",
              "      <td>0.370832</td>\n",
              "      <td>0.206664</td>\n",
              "      <td>0.177707</td>\n",
              "      <td>0.010305</td>\n",
              "      <td>0.154637</td>\n",
              "      <td>-0.127650</td>\n",
              "      <td>0.091875</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.103888</td>\n",
              "      <td>0.030449</td>\n",
              "      <td>-0.072375</td>\n",
              "      <td>0.084060</td>\n",
              "      <td>-0.200314</td>\n",
              "      <td>-0.173415</td>\n",
              "      <td>-0.078824</td>\n",
              "      <td>0.032272</td>\n",
              "      <td>-0.327215</td>\n",
              "      <td>0.246959</td>\n",
              "      <td>0.092916</td>\n",
              "      <td>0.136684</td>\n",
              "      <td>0.052101</td>\n",
              "      <td>-0.117799</td>\n",
              "      <td>-0.097961</td>\n",
              "      <td>0.025057</td>\n",
              "      <td>-0.079914</td>\n",
              "      <td>0.003686</td>\n",
              "      <td>0.017816</td>\n",
              "      <td>0.162660</td>\n",
              "      <td>0.151691</td>\n",
              "      <td>-0.169811</td>\n",
              "      <td>-0.286977</td>\n",
              "      <td>0.046300</td>\n",
              "      <td>0.162457</td>\n",
              "      <td>-0.068755</td>\n",
              "      <td>-0.324309</td>\n",
              "      <td>0.198030</td>\n",
              "      <td>-0.074244</td>\n",
              "      <td>0.033139</td>\n",
              "      <td>0.059784</td>\n",
              "      <td>-0.128351</td>\n",
              "      <td>0.217884</td>\n",
              "      <td>0.064765</td>\n",
              "      <td>-0.019849</td>\n",
              "      <td>-0.094946</td>\n",
              "      <td>0.025600</td>\n",
              "      <td>0.126472</td>\n",
              "      <td>-0.141238</td>\n",
              "      <td>-0.083810</td>\n",
              "      <td>0.059785</td>\n",
              "      <td>-0.105976</td>\n",
              "      <td>0.078958</td>\n",
              "      <td>-0.133958</td>\n",
              "      <td>0.125615</td>\n",
              "      <td>0.002990</td>\n",
              "      <td>-0.087544</td>\n",
              "      <td>-0.195172</td>\n",
              "      <td>-0.035026</td>\n",
              "      <td>0.087696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.164385</td>\n",
              "      <td>-0.043434</td>\n",
              "      <td>-0.084495</td>\n",
              "      <td>0.152406</td>\n",
              "      <td>0.285499</td>\n",
              "      <td>-0.055911</td>\n",
              "      <td>-0.150722</td>\n",
              "      <td>-0.010931</td>\n",
              "      <td>0.013554</td>\n",
              "      <td>0.154456</td>\n",
              "      <td>-0.036234</td>\n",
              "      <td>0.086589</td>\n",
              "      <td>-0.000526</td>\n",
              "      <td>0.146970</td>\n",
              "      <td>0.243333</td>\n",
              "      <td>-0.020292</td>\n",
              "      <td>-0.083187</td>\n",
              "      <td>-0.115039</td>\n",
              "      <td>0.111363</td>\n",
              "      <td>-0.049786</td>\n",
              "      <td>-0.040622</td>\n",
              "      <td>0.077256</td>\n",
              "      <td>0.051399</td>\n",
              "      <td>-0.090095</td>\n",
              "      <td>-0.247410</td>\n",
              "      <td>-0.076487</td>\n",
              "      <td>-0.276273</td>\n",
              "      <td>0.130836</td>\n",
              "      <td>-0.033362</td>\n",
              "      <td>0.319014</td>\n",
              "      <td>0.196961</td>\n",
              "      <td>0.027937</td>\n",
              "      <td>0.070903</td>\n",
              "      <td>-0.217655</td>\n",
              "      <td>-0.049470</td>\n",
              "      <td>0.013631</td>\n",
              "      <td>-0.051026</td>\n",
              "      <td>0.342463</td>\n",
              "      <td>-0.061035</td>\n",
              "      <td>0.008113</td>\n",
              "      <td>0.094657</td>\n",
              "      <td>-0.060770</td>\n",
              "      <td>0.160620</td>\n",
              "      <td>0.370619</td>\n",
              "      <td>0.206669</td>\n",
              "      <td>0.177698</td>\n",
              "      <td>0.010468</td>\n",
              "      <td>0.154517</td>\n",
              "      <td>-0.127518</td>\n",
              "      <td>0.091881</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.103818</td>\n",
              "      <td>0.030347</td>\n",
              "      <td>-0.072356</td>\n",
              "      <td>0.084055</td>\n",
              "      <td>-0.200269</td>\n",
              "      <td>-0.173422</td>\n",
              "      <td>-0.078786</td>\n",
              "      <td>0.032160</td>\n",
              "      <td>-0.327094</td>\n",
              "      <td>0.246769</td>\n",
              "      <td>0.092836</td>\n",
              "      <td>0.136521</td>\n",
              "      <td>0.052175</td>\n",
              "      <td>-0.117762</td>\n",
              "      <td>-0.097835</td>\n",
              "      <td>0.024982</td>\n",
              "      <td>-0.079784</td>\n",
              "      <td>0.003654</td>\n",
              "      <td>0.017685</td>\n",
              "      <td>0.162680</td>\n",
              "      <td>0.151633</td>\n",
              "      <td>-0.169867</td>\n",
              "      <td>-0.286736</td>\n",
              "      <td>0.046273</td>\n",
              "      <td>0.162390</td>\n",
              "      <td>-0.068749</td>\n",
              "      <td>-0.324112</td>\n",
              "      <td>0.198118</td>\n",
              "      <td>-0.074206</td>\n",
              "      <td>0.033185</td>\n",
              "      <td>0.059790</td>\n",
              "      <td>-0.128185</td>\n",
              "      <td>0.217637</td>\n",
              "      <td>0.064824</td>\n",
              "      <td>-0.019906</td>\n",
              "      <td>-0.094876</td>\n",
              "      <td>0.025602</td>\n",
              "      <td>0.126366</td>\n",
              "      <td>-0.141211</td>\n",
              "      <td>-0.083907</td>\n",
              "      <td>0.059759</td>\n",
              "      <td>-0.105940</td>\n",
              "      <td>0.078900</td>\n",
              "      <td>-0.133892</td>\n",
              "      <td>0.125618</td>\n",
              "      <td>0.002928</td>\n",
              "      <td>-0.087578</td>\n",
              "      <td>-0.195038</td>\n",
              "      <td>-0.035052</td>\n",
              "      <td>0.087563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.164230</td>\n",
              "      <td>-0.043366</td>\n",
              "      <td>-0.084491</td>\n",
              "      <td>0.152406</td>\n",
              "      <td>0.285473</td>\n",
              "      <td>-0.055815</td>\n",
              "      <td>-0.150695</td>\n",
              "      <td>-0.010900</td>\n",
              "      <td>0.013571</td>\n",
              "      <td>0.154358</td>\n",
              "      <td>-0.036202</td>\n",
              "      <td>0.086603</td>\n",
              "      <td>-0.000603</td>\n",
              "      <td>0.146846</td>\n",
              "      <td>0.243240</td>\n",
              "      <td>-0.020287</td>\n",
              "      <td>-0.083140</td>\n",
              "      <td>-0.114977</td>\n",
              "      <td>0.111263</td>\n",
              "      <td>-0.049709</td>\n",
              "      <td>-0.040576</td>\n",
              "      <td>0.077152</td>\n",
              "      <td>0.051331</td>\n",
              "      <td>-0.090159</td>\n",
              "      <td>-0.247260</td>\n",
              "      <td>-0.076426</td>\n",
              "      <td>-0.276085</td>\n",
              "      <td>0.130785</td>\n",
              "      <td>-0.033297</td>\n",
              "      <td>0.318764</td>\n",
              "      <td>0.196902</td>\n",
              "      <td>0.027935</td>\n",
              "      <td>0.070932</td>\n",
              "      <td>-0.217565</td>\n",
              "      <td>-0.049452</td>\n",
              "      <td>0.013629</td>\n",
              "      <td>-0.051028</td>\n",
              "      <td>0.342313</td>\n",
              "      <td>-0.060947</td>\n",
              "      <td>0.008252</td>\n",
              "      <td>0.094636</td>\n",
              "      <td>-0.060675</td>\n",
              "      <td>0.160725</td>\n",
              "      <td>0.370406</td>\n",
              "      <td>0.206674</td>\n",
              "      <td>0.177689</td>\n",
              "      <td>0.010630</td>\n",
              "      <td>0.154396</td>\n",
              "      <td>-0.127385</td>\n",
              "      <td>0.091888</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.103748</td>\n",
              "      <td>0.030245</td>\n",
              "      <td>-0.072337</td>\n",
              "      <td>0.084051</td>\n",
              "      <td>-0.200223</td>\n",
              "      <td>-0.173429</td>\n",
              "      <td>-0.078748</td>\n",
              "      <td>0.032048</td>\n",
              "      <td>-0.326972</td>\n",
              "      <td>0.246579</td>\n",
              "      <td>0.092756</td>\n",
              "      <td>0.136358</td>\n",
              "      <td>0.052249</td>\n",
              "      <td>-0.117725</td>\n",
              "      <td>-0.097708</td>\n",
              "      <td>0.024906</td>\n",
              "      <td>-0.079653</td>\n",
              "      <td>0.003623</td>\n",
              "      <td>0.017555</td>\n",
              "      <td>0.162700</td>\n",
              "      <td>0.151575</td>\n",
              "      <td>-0.169924</td>\n",
              "      <td>-0.286495</td>\n",
              "      <td>0.046246</td>\n",
              "      <td>0.162322</td>\n",
              "      <td>-0.068743</td>\n",
              "      <td>-0.323914</td>\n",
              "      <td>0.198207</td>\n",
              "      <td>-0.074169</td>\n",
              "      <td>0.033232</td>\n",
              "      <td>0.059796</td>\n",
              "      <td>-0.128019</td>\n",
              "      <td>0.217390</td>\n",
              "      <td>0.064883</td>\n",
              "      <td>-0.019963</td>\n",
              "      <td>-0.094807</td>\n",
              "      <td>0.025603</td>\n",
              "      <td>0.126259</td>\n",
              "      <td>-0.141184</td>\n",
              "      <td>-0.084005</td>\n",
              "      <td>0.059732</td>\n",
              "      <td>-0.105905</td>\n",
              "      <td>0.078842</td>\n",
              "      <td>-0.133826</td>\n",
              "      <td>0.125621</td>\n",
              "      <td>0.002866</td>\n",
              "      <td>-0.087613</td>\n",
              "      <td>-0.194904</td>\n",
              "      <td>-0.035079</td>\n",
              "      <td>0.087431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.164075</td>\n",
              "      <td>-0.043297</td>\n",
              "      <td>-0.084487</td>\n",
              "      <td>0.152406</td>\n",
              "      <td>0.285448</td>\n",
              "      <td>-0.055720</td>\n",
              "      <td>-0.150668</td>\n",
              "      <td>-0.010869</td>\n",
              "      <td>0.013589</td>\n",
              "      <td>0.154261</td>\n",
              "      <td>-0.036170</td>\n",
              "      <td>0.086616</td>\n",
              "      <td>-0.000679</td>\n",
              "      <td>0.146722</td>\n",
              "      <td>0.243147</td>\n",
              "      <td>-0.020282</td>\n",
              "      <td>-0.083093</td>\n",
              "      <td>-0.114915</td>\n",
              "      <td>0.111163</td>\n",
              "      <td>-0.049632</td>\n",
              "      <td>-0.040531</td>\n",
              "      <td>0.077047</td>\n",
              "      <td>0.051263</td>\n",
              "      <td>-0.090222</td>\n",
              "      <td>-0.247110</td>\n",
              "      <td>-0.076364</td>\n",
              "      <td>-0.275897</td>\n",
              "      <td>0.130735</td>\n",
              "      <td>-0.033233</td>\n",
              "      <td>0.318515</td>\n",
              "      <td>0.196843</td>\n",
              "      <td>0.027932</td>\n",
              "      <td>0.070961</td>\n",
              "      <td>-0.217476</td>\n",
              "      <td>-0.049434</td>\n",
              "      <td>0.013627</td>\n",
              "      <td>-0.051030</td>\n",
              "      <td>0.342163</td>\n",
              "      <td>-0.060859</td>\n",
              "      <td>0.008390</td>\n",
              "      <td>0.094615</td>\n",
              "      <td>-0.060580</td>\n",
              "      <td>0.160830</td>\n",
              "      <td>0.370194</td>\n",
              "      <td>0.206678</td>\n",
              "      <td>0.177680</td>\n",
              "      <td>0.010792</td>\n",
              "      <td>0.154276</td>\n",
              "      <td>-0.127253</td>\n",
              "      <td>0.091894</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.103678</td>\n",
              "      <td>0.030143</td>\n",
              "      <td>-0.072318</td>\n",
              "      <td>0.084047</td>\n",
              "      <td>-0.200178</td>\n",
              "      <td>-0.173436</td>\n",
              "      <td>-0.078711</td>\n",
              "      <td>0.031936</td>\n",
              "      <td>-0.326851</td>\n",
              "      <td>0.246389</td>\n",
              "      <td>0.092676</td>\n",
              "      <td>0.136194</td>\n",
              "      <td>0.052323</td>\n",
              "      <td>-0.117689</td>\n",
              "      <td>-0.097581</td>\n",
              "      <td>0.024831</td>\n",
              "      <td>-0.079523</td>\n",
              "      <td>0.003592</td>\n",
              "      <td>0.017424</td>\n",
              "      <td>0.162720</td>\n",
              "      <td>0.151516</td>\n",
              "      <td>-0.169981</td>\n",
              "      <td>-0.286254</td>\n",
              "      <td>0.046220</td>\n",
              "      <td>0.162255</td>\n",
              "      <td>-0.068738</td>\n",
              "      <td>-0.323717</td>\n",
              "      <td>0.198295</td>\n",
              "      <td>-0.074132</td>\n",
              "      <td>0.033278</td>\n",
              "      <td>0.059802</td>\n",
              "      <td>-0.127853</td>\n",
              "      <td>0.217143</td>\n",
              "      <td>0.064942</td>\n",
              "      <td>-0.020020</td>\n",
              "      <td>-0.094737</td>\n",
              "      <td>0.025605</td>\n",
              "      <td>0.126153</td>\n",
              "      <td>-0.141157</td>\n",
              "      <td>-0.084102</td>\n",
              "      <td>0.059705</td>\n",
              "      <td>-0.105869</td>\n",
              "      <td>0.078785</td>\n",
              "      <td>-0.133761</td>\n",
              "      <td>0.125624</td>\n",
              "      <td>0.002805</td>\n",
              "      <td>-0.087647</td>\n",
              "      <td>-0.194770</td>\n",
              "      <td>-0.035105</td>\n",
              "      <td>0.087299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.163920</td>\n",
              "      <td>-0.043229</td>\n",
              "      <td>-0.084483</td>\n",
              "      <td>0.152407</td>\n",
              "      <td>0.285422</td>\n",
              "      <td>-0.055625</td>\n",
              "      <td>-0.150642</td>\n",
              "      <td>-0.010838</td>\n",
              "      <td>0.013606</td>\n",
              "      <td>0.154163</td>\n",
              "      <td>-0.036137</td>\n",
              "      <td>0.086629</td>\n",
              "      <td>-0.000756</td>\n",
              "      <td>0.146599</td>\n",
              "      <td>0.243054</td>\n",
              "      <td>-0.020277</td>\n",
              "      <td>-0.083046</td>\n",
              "      <td>-0.114853</td>\n",
              "      <td>0.111063</td>\n",
              "      <td>-0.049554</td>\n",
              "      <td>-0.040486</td>\n",
              "      <td>0.076943</td>\n",
              "      <td>0.051196</td>\n",
              "      <td>-0.090286</td>\n",
              "      <td>-0.246960</td>\n",
              "      <td>-0.076303</td>\n",
              "      <td>-0.275709</td>\n",
              "      <td>0.130684</td>\n",
              "      <td>-0.033168</td>\n",
              "      <td>0.318265</td>\n",
              "      <td>0.196785</td>\n",
              "      <td>0.027929</td>\n",
              "      <td>0.070990</td>\n",
              "      <td>-0.217386</td>\n",
              "      <td>-0.049417</td>\n",
              "      <td>0.013625</td>\n",
              "      <td>-0.051033</td>\n",
              "      <td>0.342013</td>\n",
              "      <td>-0.060771</td>\n",
              "      <td>0.008529</td>\n",
              "      <td>0.094594</td>\n",
              "      <td>-0.060485</td>\n",
              "      <td>0.160935</td>\n",
              "      <td>0.369981</td>\n",
              "      <td>0.206683</td>\n",
              "      <td>0.177671</td>\n",
              "      <td>0.010954</td>\n",
              "      <td>0.154155</td>\n",
              "      <td>-0.127121</td>\n",
              "      <td>0.091901</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.103608</td>\n",
              "      <td>0.030041</td>\n",
              "      <td>-0.072299</td>\n",
              "      <td>0.084043</td>\n",
              "      <td>-0.200132</td>\n",
              "      <td>-0.173443</td>\n",
              "      <td>-0.078673</td>\n",
              "      <td>0.031824</td>\n",
              "      <td>-0.326730</td>\n",
              "      <td>0.246200</td>\n",
              "      <td>0.092597</td>\n",
              "      <td>0.136031</td>\n",
              "      <td>0.052397</td>\n",
              "      <td>-0.117652</td>\n",
              "      <td>-0.097454</td>\n",
              "      <td>0.024755</td>\n",
              "      <td>-0.079393</td>\n",
              "      <td>0.003561</td>\n",
              "      <td>0.017294</td>\n",
              "      <td>0.162740</td>\n",
              "      <td>0.151458</td>\n",
              "      <td>-0.170037</td>\n",
              "      <td>-0.286013</td>\n",
              "      <td>0.046193</td>\n",
              "      <td>0.162188</td>\n",
              "      <td>-0.068732</td>\n",
              "      <td>-0.323519</td>\n",
              "      <td>0.198384</td>\n",
              "      <td>-0.074095</td>\n",
              "      <td>0.033325</td>\n",
              "      <td>0.059809</td>\n",
              "      <td>-0.127687</td>\n",
              "      <td>0.216896</td>\n",
              "      <td>0.065000</td>\n",
              "      <td>-0.020076</td>\n",
              "      <td>-0.094668</td>\n",
              "      <td>0.025606</td>\n",
              "      <td>0.126047</td>\n",
              "      <td>-0.141130</td>\n",
              "      <td>-0.084199</td>\n",
              "      <td>0.059679</td>\n",
              "      <td>-0.105834</td>\n",
              "      <td>0.078727</td>\n",
              "      <td>-0.133695</td>\n",
              "      <td>0.125626</td>\n",
              "      <td>0.002743</td>\n",
              "      <td>-0.087681</td>\n",
              "      <td>-0.194636</td>\n",
              "      <td>-0.035132</td>\n",
              "      <td>0.087167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>0.144576</td>\n",
              "      <td>0.093070</td>\n",
              "      <td>-0.076889</td>\n",
              "      <td>0.152829</td>\n",
              "      <td>0.234230</td>\n",
              "      <td>0.134305</td>\n",
              "      <td>-0.097537</td>\n",
              "      <td>0.051038</td>\n",
              "      <td>0.048360</td>\n",
              "      <td>-0.040125</td>\n",
              "      <td>0.028359</td>\n",
              "      <td>0.112987</td>\n",
              "      <td>-0.153232</td>\n",
              "      <td>-0.099835</td>\n",
              "      <td>0.057713</td>\n",
              "      <td>-0.009798</td>\n",
              "      <td>0.011040</td>\n",
              "      <td>0.008897</td>\n",
              "      <td>-0.087892</td>\n",
              "      <td>0.104113</td>\n",
              "      <td>0.049499</td>\n",
              "      <td>-0.130745</td>\n",
              "      <td>-0.083445</td>\n",
              "      <td>-0.216949</td>\n",
              "      <td>0.052103</td>\n",
              "      <td>0.045655</td>\n",
              "      <td>0.098523</td>\n",
              "      <td>0.029759</td>\n",
              "      <td>0.096039</td>\n",
              "      <td>-0.178363</td>\n",
              "      <td>0.080131</td>\n",
              "      <td>0.022465</td>\n",
              "      <td>0.129092</td>\n",
              "      <td>-0.039076</td>\n",
              "      <td>-0.014258</td>\n",
              "      <td>0.009589</td>\n",
              "      <td>-0.055605</td>\n",
              "      <td>0.043675</td>\n",
              "      <td>0.114132</td>\n",
              "      <td>0.284316</td>\n",
              "      <td>0.052660</td>\n",
              "      <td>0.128189</td>\n",
              "      <td>0.369619</td>\n",
              "      <td>-0.053489</td>\n",
              "      <td>0.215722</td>\n",
              "      <td>0.159651</td>\n",
              "      <td>0.334012</td>\n",
              "      <td>-0.085641</td>\n",
              "      <td>0.136309</td>\n",
              "      <td>0.104756</td>\n",
              "      <td>...</td>\n",
              "      <td>0.035964</td>\n",
              "      <td>-0.173130</td>\n",
              "      <td>-0.034843</td>\n",
              "      <td>0.075910</td>\n",
              "      <td>-0.109507</td>\n",
              "      <td>-0.187538</td>\n",
              "      <td>-0.003215</td>\n",
              "      <td>-0.190824</td>\n",
              "      <td>-0.085187</td>\n",
              "      <td>-0.131676</td>\n",
              "      <td>-0.066359</td>\n",
              "      <td>-0.188736</td>\n",
              "      <td>0.199669</td>\n",
              "      <td>-0.044898</td>\n",
              "      <td>0.154852</td>\n",
              "      <td>-0.125500</td>\n",
              "      <td>0.180192</td>\n",
              "      <td>-0.058423</td>\n",
              "      <td>-0.242590</td>\n",
              "      <td>0.202217</td>\n",
              "      <td>0.035215</td>\n",
              "      <td>-0.282732</td>\n",
              "      <td>0.194125</td>\n",
              "      <td>-0.007162</td>\n",
              "      <td>0.028559</td>\n",
              "      <td>-0.057462</td>\n",
              "      <td>0.069736</td>\n",
              "      <td>0.374694</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>0.125860</td>\n",
              "      <td>0.072188</td>\n",
              "      <td>0.202831</td>\n",
              "      <td>-0.274979</td>\n",
              "      <td>0.181876</td>\n",
              "      <td>-0.133127</td>\n",
              "      <td>0.043893</td>\n",
              "      <td>0.028344</td>\n",
              "      <td>-0.085819</td>\n",
              "      <td>-0.087385</td>\n",
              "      <td>-0.278012</td>\n",
              "      <td>0.006700</td>\n",
              "      <td>-0.035158</td>\n",
              "      <td>-0.036274</td>\n",
              "      <td>-0.002642</td>\n",
              "      <td>0.131440</td>\n",
              "      <td>-0.119885</td>\n",
              "      <td>-0.156117</td>\n",
              "      <td>0.072136</td>\n",
              "      <td>-0.087926</td>\n",
              "      <td>-0.175866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>0.144731</td>\n",
              "      <td>0.093139</td>\n",
              "      <td>-0.076885</td>\n",
              "      <td>0.152829</td>\n",
              "      <td>0.234205</td>\n",
              "      <td>0.134400</td>\n",
              "      <td>-0.097510</td>\n",
              "      <td>0.051069</td>\n",
              "      <td>0.048377</td>\n",
              "      <td>-0.040222</td>\n",
              "      <td>0.028391</td>\n",
              "      <td>0.113000</td>\n",
              "      <td>-0.153309</td>\n",
              "      <td>-0.099959</td>\n",
              "      <td>0.057620</td>\n",
              "      <td>-0.009792</td>\n",
              "      <td>0.011087</td>\n",
              "      <td>0.008959</td>\n",
              "      <td>-0.087992</td>\n",
              "      <td>0.104190</td>\n",
              "      <td>0.049544</td>\n",
              "      <td>-0.130849</td>\n",
              "      <td>-0.083513</td>\n",
              "      <td>-0.217013</td>\n",
              "      <td>0.052253</td>\n",
              "      <td>0.045716</td>\n",
              "      <td>0.098711</td>\n",
              "      <td>0.029708</td>\n",
              "      <td>0.096104</td>\n",
              "      <td>-0.178613</td>\n",
              "      <td>0.080073</td>\n",
              "      <td>0.022463</td>\n",
              "      <td>0.129121</td>\n",
              "      <td>-0.038987</td>\n",
              "      <td>-0.014240</td>\n",
              "      <td>0.009587</td>\n",
              "      <td>-0.055607</td>\n",
              "      <td>0.043525</td>\n",
              "      <td>0.114220</td>\n",
              "      <td>0.284454</td>\n",
              "      <td>0.052639</td>\n",
              "      <td>0.128284</td>\n",
              "      <td>0.369723</td>\n",
              "      <td>-0.053702</td>\n",
              "      <td>0.215727</td>\n",
              "      <td>0.159642</td>\n",
              "      <td>0.334175</td>\n",
              "      <td>-0.085762</td>\n",
              "      <td>0.136442</td>\n",
              "      <td>0.104762</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036034</td>\n",
              "      <td>-0.173232</td>\n",
              "      <td>-0.034824</td>\n",
              "      <td>0.075906</td>\n",
              "      <td>-0.109461</td>\n",
              "      <td>-0.187546</td>\n",
              "      <td>-0.003177</td>\n",
              "      <td>-0.190936</td>\n",
              "      <td>-0.085066</td>\n",
              "      <td>-0.131866</td>\n",
              "      <td>-0.066439</td>\n",
              "      <td>-0.188899</td>\n",
              "      <td>0.199743</td>\n",
              "      <td>-0.044862</td>\n",
              "      <td>0.154979</td>\n",
              "      <td>-0.125575</td>\n",
              "      <td>0.180322</td>\n",
              "      <td>-0.058454</td>\n",
              "      <td>-0.242720</td>\n",
              "      <td>0.202237</td>\n",
              "      <td>0.035157</td>\n",
              "      <td>-0.282788</td>\n",
              "      <td>0.194366</td>\n",
              "      <td>-0.007188</td>\n",
              "      <td>0.028492</td>\n",
              "      <td>-0.057456</td>\n",
              "      <td>0.069934</td>\n",
              "      <td>0.374783</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.125906</td>\n",
              "      <td>0.072194</td>\n",
              "      <td>0.202997</td>\n",
              "      <td>-0.275226</td>\n",
              "      <td>0.181935</td>\n",
              "      <td>-0.133183</td>\n",
              "      <td>0.043962</td>\n",
              "      <td>0.028345</td>\n",
              "      <td>-0.085926</td>\n",
              "      <td>-0.087358</td>\n",
              "      <td>-0.278109</td>\n",
              "      <td>0.006674</td>\n",
              "      <td>-0.035123</td>\n",
              "      <td>-0.036332</td>\n",
              "      <td>-0.002576</td>\n",
              "      <td>0.131443</td>\n",
              "      <td>-0.119946</td>\n",
              "      <td>-0.156151</td>\n",
              "      <td>0.072270</td>\n",
              "      <td>-0.087953</td>\n",
              "      <td>-0.175998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>0.144886</td>\n",
              "      <td>0.093207</td>\n",
              "      <td>-0.076881</td>\n",
              "      <td>0.152830</td>\n",
              "      <td>0.234179</td>\n",
              "      <td>0.134496</td>\n",
              "      <td>-0.097484</td>\n",
              "      <td>0.051100</td>\n",
              "      <td>0.048395</td>\n",
              "      <td>-0.040320</td>\n",
              "      <td>0.028424</td>\n",
              "      <td>0.113013</td>\n",
              "      <td>-0.153385</td>\n",
              "      <td>-0.100082</td>\n",
              "      <td>0.057526</td>\n",
              "      <td>-0.009787</td>\n",
              "      <td>0.011135</td>\n",
              "      <td>0.009021</td>\n",
              "      <td>-0.088092</td>\n",
              "      <td>0.104268</td>\n",
              "      <td>0.049589</td>\n",
              "      <td>-0.130954</td>\n",
              "      <td>-0.083580</td>\n",
              "      <td>-0.217076</td>\n",
              "      <td>0.052403</td>\n",
              "      <td>0.045778</td>\n",
              "      <td>0.098899</td>\n",
              "      <td>0.029658</td>\n",
              "      <td>0.096169</td>\n",
              "      <td>-0.178862</td>\n",
              "      <td>0.080014</td>\n",
              "      <td>0.022460</td>\n",
              "      <td>0.129150</td>\n",
              "      <td>-0.038897</td>\n",
              "      <td>-0.014223</td>\n",
              "      <td>0.009585</td>\n",
              "      <td>-0.055609</td>\n",
              "      <td>0.043375</td>\n",
              "      <td>0.114308</td>\n",
              "      <td>0.284593</td>\n",
              "      <td>0.052618</td>\n",
              "      <td>0.128378</td>\n",
              "      <td>0.369828</td>\n",
              "      <td>-0.053915</td>\n",
              "      <td>0.215732</td>\n",
              "      <td>0.159633</td>\n",
              "      <td>0.334337</td>\n",
              "      <td>-0.085882</td>\n",
              "      <td>0.136574</td>\n",
              "      <td>0.104769</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036104</td>\n",
              "      <td>-0.173334</td>\n",
              "      <td>-0.034805</td>\n",
              "      <td>0.075902</td>\n",
              "      <td>-0.109416</td>\n",
              "      <td>-0.187553</td>\n",
              "      <td>-0.003139</td>\n",
              "      <td>-0.191048</td>\n",
              "      <td>-0.084945</td>\n",
              "      <td>-0.132056</td>\n",
              "      <td>-0.066519</td>\n",
              "      <td>-0.189063</td>\n",
              "      <td>0.199817</td>\n",
              "      <td>-0.044825</td>\n",
              "      <td>0.155105</td>\n",
              "      <td>-0.125651</td>\n",
              "      <td>0.180453</td>\n",
              "      <td>-0.058485</td>\n",
              "      <td>-0.242851</td>\n",
              "      <td>0.202256</td>\n",
              "      <td>0.035099</td>\n",
              "      <td>-0.282845</td>\n",
              "      <td>0.194607</td>\n",
              "      <td>-0.007215</td>\n",
              "      <td>0.028424</td>\n",
              "      <td>-0.057450</td>\n",
              "      <td>0.070132</td>\n",
              "      <td>0.374871</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.125953</td>\n",
              "      <td>0.072200</td>\n",
              "      <td>0.203163</td>\n",
              "      <td>-0.275473</td>\n",
              "      <td>0.181994</td>\n",
              "      <td>-0.133240</td>\n",
              "      <td>0.044032</td>\n",
              "      <td>0.028346</td>\n",
              "      <td>-0.086032</td>\n",
              "      <td>-0.087331</td>\n",
              "      <td>-0.278206</td>\n",
              "      <td>0.006647</td>\n",
              "      <td>-0.035087</td>\n",
              "      <td>-0.036390</td>\n",
              "      <td>-0.002511</td>\n",
              "      <td>0.131445</td>\n",
              "      <td>-0.120008</td>\n",
              "      <td>-0.156185</td>\n",
              "      <td>0.072404</td>\n",
              "      <td>-0.087980</td>\n",
              "      <td>-0.176130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>0.145041</td>\n",
              "      <td>0.093276</td>\n",
              "      <td>-0.076877</td>\n",
              "      <td>0.152830</td>\n",
              "      <td>0.234153</td>\n",
              "      <td>0.134591</td>\n",
              "      <td>-0.097457</td>\n",
              "      <td>0.051131</td>\n",
              "      <td>0.048412</td>\n",
              "      <td>-0.040417</td>\n",
              "      <td>0.028456</td>\n",
              "      <td>0.113027</td>\n",
              "      <td>-0.153462</td>\n",
              "      <td>-0.100206</td>\n",
              "      <td>0.057433</td>\n",
              "      <td>-0.009782</td>\n",
              "      <td>0.011182</td>\n",
              "      <td>0.009083</td>\n",
              "      <td>-0.088192</td>\n",
              "      <td>0.104345</td>\n",
              "      <td>0.049634</td>\n",
              "      <td>-0.131058</td>\n",
              "      <td>-0.083648</td>\n",
              "      <td>-0.217140</td>\n",
              "      <td>0.052553</td>\n",
              "      <td>0.045839</td>\n",
              "      <td>0.099086</td>\n",
              "      <td>0.029607</td>\n",
              "      <td>0.096234</td>\n",
              "      <td>-0.179112</td>\n",
              "      <td>0.079956</td>\n",
              "      <td>0.022457</td>\n",
              "      <td>0.129179</td>\n",
              "      <td>-0.038808</td>\n",
              "      <td>-0.014205</td>\n",
              "      <td>0.009583</td>\n",
              "      <td>-0.055612</td>\n",
              "      <td>0.043225</td>\n",
              "      <td>0.114396</td>\n",
              "      <td>0.284732</td>\n",
              "      <td>0.052597</td>\n",
              "      <td>0.128473</td>\n",
              "      <td>0.369933</td>\n",
              "      <td>-0.054127</td>\n",
              "      <td>0.215736</td>\n",
              "      <td>0.159624</td>\n",
              "      <td>0.334499</td>\n",
              "      <td>-0.086003</td>\n",
              "      <td>0.136706</td>\n",
              "      <td>0.104775</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036174</td>\n",
              "      <td>-0.173436</td>\n",
              "      <td>-0.034787</td>\n",
              "      <td>0.075898</td>\n",
              "      <td>-0.109370</td>\n",
              "      <td>-0.187560</td>\n",
              "      <td>-0.003101</td>\n",
              "      <td>-0.191160</td>\n",
              "      <td>-0.084823</td>\n",
              "      <td>-0.132245</td>\n",
              "      <td>-0.066599</td>\n",
              "      <td>-0.189226</td>\n",
              "      <td>0.199891</td>\n",
              "      <td>-0.044789</td>\n",
              "      <td>0.155232</td>\n",
              "      <td>-0.125726</td>\n",
              "      <td>0.180583</td>\n",
              "      <td>-0.058516</td>\n",
              "      <td>-0.242981</td>\n",
              "      <td>0.202276</td>\n",
              "      <td>0.035040</td>\n",
              "      <td>-0.282902</td>\n",
              "      <td>0.194849</td>\n",
              "      <td>-0.007242</td>\n",
              "      <td>0.028357</td>\n",
              "      <td>-0.057445</td>\n",
              "      <td>0.070329</td>\n",
              "      <td>0.374960</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.125999</td>\n",
              "      <td>0.072207</td>\n",
              "      <td>0.203329</td>\n",
              "      <td>-0.275720</td>\n",
              "      <td>0.182052</td>\n",
              "      <td>-0.133297</td>\n",
              "      <td>0.044101</td>\n",
              "      <td>0.028348</td>\n",
              "      <td>-0.086139</td>\n",
              "      <td>-0.087304</td>\n",
              "      <td>-0.278304</td>\n",
              "      <td>0.006621</td>\n",
              "      <td>-0.035052</td>\n",
              "      <td>-0.036447</td>\n",
              "      <td>-0.002445</td>\n",
              "      <td>0.131448</td>\n",
              "      <td>-0.120070</td>\n",
              "      <td>-0.156220</td>\n",
              "      <td>0.072538</td>\n",
              "      <td>-0.088006</td>\n",
              "      <td>-0.176262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>0.145196</td>\n",
              "      <td>0.093344</td>\n",
              "      <td>-0.076874</td>\n",
              "      <td>0.152830</td>\n",
              "      <td>0.234128</td>\n",
              "      <td>0.134686</td>\n",
              "      <td>-0.097430</td>\n",
              "      <td>0.051162</td>\n",
              "      <td>0.048430</td>\n",
              "      <td>-0.040515</td>\n",
              "      <td>0.028488</td>\n",
              "      <td>0.113040</td>\n",
              "      <td>-0.153539</td>\n",
              "      <td>-0.100330</td>\n",
              "      <td>0.057340</td>\n",
              "      <td>-0.009776</td>\n",
              "      <td>0.011229</td>\n",
              "      <td>0.009145</td>\n",
              "      <td>-0.088292</td>\n",
              "      <td>0.104422</td>\n",
              "      <td>0.049680</td>\n",
              "      <td>-0.131162</td>\n",
              "      <td>-0.083716</td>\n",
              "      <td>-0.217204</td>\n",
              "      <td>0.052703</td>\n",
              "      <td>0.045900</td>\n",
              "      <td>0.099274</td>\n",
              "      <td>0.029556</td>\n",
              "      <td>0.096299</td>\n",
              "      <td>-0.179361</td>\n",
              "      <td>0.079897</td>\n",
              "      <td>0.022454</td>\n",
              "      <td>0.129208</td>\n",
              "      <td>-0.038718</td>\n",
              "      <td>-0.014187</td>\n",
              "      <td>0.009581</td>\n",
              "      <td>-0.055614</td>\n",
              "      <td>0.043075</td>\n",
              "      <td>0.114484</td>\n",
              "      <td>0.284870</td>\n",
              "      <td>0.052576</td>\n",
              "      <td>0.128568</td>\n",
              "      <td>0.370038</td>\n",
              "      <td>-0.054340</td>\n",
              "      <td>0.215741</td>\n",
              "      <td>0.159615</td>\n",
              "      <td>0.334661</td>\n",
              "      <td>-0.086123</td>\n",
              "      <td>0.136839</td>\n",
              "      <td>0.104781</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036244</td>\n",
              "      <td>-0.173538</td>\n",
              "      <td>-0.034768</td>\n",
              "      <td>0.075894</td>\n",
              "      <td>-0.109325</td>\n",
              "      <td>-0.187567</td>\n",
              "      <td>-0.003063</td>\n",
              "      <td>-0.191271</td>\n",
              "      <td>-0.084702</td>\n",
              "      <td>-0.132435</td>\n",
              "      <td>-0.066679</td>\n",
              "      <td>-0.189389</td>\n",
              "      <td>0.199965</td>\n",
              "      <td>-0.044752</td>\n",
              "      <td>0.155359</td>\n",
              "      <td>-0.125802</td>\n",
              "      <td>0.180714</td>\n",
              "      <td>-0.058547</td>\n",
              "      <td>-0.243112</td>\n",
              "      <td>0.202296</td>\n",
              "      <td>0.034982</td>\n",
              "      <td>-0.282958</td>\n",
              "      <td>0.195090</td>\n",
              "      <td>-0.007269</td>\n",
              "      <td>0.028290</td>\n",
              "      <td>-0.057439</td>\n",
              "      <td>0.070527</td>\n",
              "      <td>0.375048</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.126046</td>\n",
              "      <td>0.072213</td>\n",
              "      <td>0.203495</td>\n",
              "      <td>-0.275968</td>\n",
              "      <td>0.182111</td>\n",
              "      <td>-0.133354</td>\n",
              "      <td>0.044171</td>\n",
              "      <td>0.028349</td>\n",
              "      <td>-0.086245</td>\n",
              "      <td>-0.087277</td>\n",
              "      <td>-0.278401</td>\n",
              "      <td>0.006594</td>\n",
              "      <td>-0.035016</td>\n",
              "      <td>-0.036505</td>\n",
              "      <td>-0.002379</td>\n",
              "      <td>0.131451</td>\n",
              "      <td>-0.120131</td>\n",
              "      <td>-0.156254</td>\n",
              "      <td>0.072672</td>\n",
              "      <td>-0.088033</td>\n",
              "      <td>-0.176394</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows  505 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0    -0.164540 -0.043503 -0.084499  0.152406  0.285525 -0.056006 -0.150748   \n",
              "1    -0.164385 -0.043434 -0.084495  0.152406  0.285499 -0.055911 -0.150722   \n",
              "2    -0.164230 -0.043366 -0.084491  0.152406  0.285473 -0.055815 -0.150695   \n",
              "3    -0.164075 -0.043297 -0.084487  0.152406  0.285448 -0.055720 -0.150668   \n",
              "4    -0.163920 -0.043229 -0.084483  0.152407  0.285422 -0.055625 -0.150642   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995  0.144576  0.093070 -0.076889  0.152829  0.234230  0.134305 -0.097537   \n",
              "1996  0.144731  0.093139 -0.076885  0.152829  0.234205  0.134400 -0.097510   \n",
              "1997  0.144886  0.093207 -0.076881  0.152830  0.234179  0.134496 -0.097484   \n",
              "1998  0.145041  0.093276 -0.076877  0.152830  0.234153  0.134591 -0.097457   \n",
              "1999  0.145196  0.093344 -0.076874  0.152830  0.234128  0.134686 -0.097430   \n",
              "\n",
              "           7         8         9         10        11        12        13   \\\n",
              "0    -0.010962  0.013536  0.154553 -0.036267  0.086576 -0.000450  0.147094   \n",
              "1    -0.010931  0.013554  0.154456 -0.036234  0.086589 -0.000526  0.146970   \n",
              "2    -0.010900  0.013571  0.154358 -0.036202  0.086603 -0.000603  0.146846   \n",
              "3    -0.010869  0.013589  0.154261 -0.036170  0.086616 -0.000679  0.146722   \n",
              "4    -0.010838  0.013606  0.154163 -0.036137  0.086629 -0.000756  0.146599   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995  0.051038  0.048360 -0.040125  0.028359  0.112987 -0.153232 -0.099835   \n",
              "1996  0.051069  0.048377 -0.040222  0.028391  0.113000 -0.153309 -0.099959   \n",
              "1997  0.051100  0.048395 -0.040320  0.028424  0.113013 -0.153385 -0.100082   \n",
              "1998  0.051131  0.048412 -0.040417  0.028456  0.113027 -0.153462 -0.100206   \n",
              "1999  0.051162  0.048430 -0.040515  0.028488  0.113040 -0.153539 -0.100330   \n",
              "\n",
              "           14        15        16        17        18        19        20   \\\n",
              "0     0.243426 -0.020298 -0.083235 -0.115102  0.111463 -0.049863 -0.040667   \n",
              "1     0.243333 -0.020292 -0.083187 -0.115039  0.111363 -0.049786 -0.040622   \n",
              "2     0.243240 -0.020287 -0.083140 -0.114977  0.111263 -0.049709 -0.040576   \n",
              "3     0.243147 -0.020282 -0.083093 -0.114915  0.111163 -0.049632 -0.040531   \n",
              "4     0.243054 -0.020277 -0.083046 -0.114853  0.111063 -0.049554 -0.040486   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995  0.057713 -0.009798  0.011040  0.008897 -0.087892  0.104113  0.049499   \n",
              "1996  0.057620 -0.009792  0.011087  0.008959 -0.087992  0.104190  0.049544   \n",
              "1997  0.057526 -0.009787  0.011135  0.009021 -0.088092  0.104268  0.049589   \n",
              "1998  0.057433 -0.009782  0.011182  0.009083 -0.088192  0.104345  0.049634   \n",
              "1999  0.057340 -0.009776  0.011229  0.009145 -0.088292  0.104422  0.049680   \n",
              "\n",
              "           21        22        23        24        25        26        27   \\\n",
              "0     0.077360  0.051466 -0.090031 -0.247560 -0.076548 -0.276460  0.130887   \n",
              "1     0.077256  0.051399 -0.090095 -0.247410 -0.076487 -0.276273  0.130836   \n",
              "2     0.077152  0.051331 -0.090159 -0.247260 -0.076426 -0.276085  0.130785   \n",
              "3     0.077047  0.051263 -0.090222 -0.247110 -0.076364 -0.275897  0.130735   \n",
              "4     0.076943  0.051196 -0.090286 -0.246960 -0.076303 -0.275709  0.130684   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995 -0.130745 -0.083445 -0.216949  0.052103  0.045655  0.098523  0.029759   \n",
              "1996 -0.130849 -0.083513 -0.217013  0.052253  0.045716  0.098711  0.029708   \n",
              "1997 -0.130954 -0.083580 -0.217076  0.052403  0.045778  0.098899  0.029658   \n",
              "1998 -0.131058 -0.083648 -0.217140  0.052553  0.045839  0.099086  0.029607   \n",
              "1999 -0.131162 -0.083716 -0.217204  0.052703  0.045900  0.099274  0.029556   \n",
              "\n",
              "           28        29        30        31        32        33        34   \\\n",
              "0    -0.033427  0.319263  0.197019  0.027940  0.070874 -0.217744 -0.049487   \n",
              "1    -0.033362  0.319014  0.196961  0.027937  0.070903 -0.217655 -0.049470   \n",
              "2    -0.033297  0.318764  0.196902  0.027935  0.070932 -0.217565 -0.049452   \n",
              "3    -0.033233  0.318515  0.196843  0.027932  0.070961 -0.217476 -0.049434   \n",
              "4    -0.033168  0.318265  0.196785  0.027929  0.070990 -0.217386 -0.049417   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995  0.096039 -0.178363  0.080131  0.022465  0.129092 -0.039076 -0.014258   \n",
              "1996  0.096104 -0.178613  0.080073  0.022463  0.129121 -0.038987 -0.014240   \n",
              "1997  0.096169 -0.178862  0.080014  0.022460  0.129150 -0.038897 -0.014223   \n",
              "1998  0.096234 -0.179112  0.079956  0.022457  0.129179 -0.038808 -0.014205   \n",
              "1999  0.096299 -0.179361  0.079897  0.022454  0.129208 -0.038718 -0.014187   \n",
              "\n",
              "           35        36        37        38        39        40        41   \\\n",
              "0     0.013633 -0.051024  0.342613 -0.061123  0.007975  0.094679 -0.060865   \n",
              "1     0.013631 -0.051026  0.342463 -0.061035  0.008113  0.094657 -0.060770   \n",
              "2     0.013629 -0.051028  0.342313 -0.060947  0.008252  0.094636 -0.060675   \n",
              "3     0.013627 -0.051030  0.342163 -0.060859  0.008390  0.094615 -0.060580   \n",
              "4     0.013625 -0.051033  0.342013 -0.060771  0.008529  0.094594 -0.060485   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995  0.009589 -0.055605  0.043675  0.114132  0.284316  0.052660  0.128189   \n",
              "1996  0.009587 -0.055607  0.043525  0.114220  0.284454  0.052639  0.128284   \n",
              "1997  0.009585 -0.055609  0.043375  0.114308  0.284593  0.052618  0.128378   \n",
              "1998  0.009583 -0.055612  0.043225  0.114396  0.284732  0.052597  0.128473   \n",
              "1999  0.009581 -0.055614  0.043075  0.114484  0.284870  0.052576  0.128568   \n",
              "\n",
              "           42        43        44        45        46        47        48   \\\n",
              "0     0.160515  0.370832  0.206664  0.177707  0.010305  0.154637 -0.127650   \n",
              "1     0.160620  0.370619  0.206669  0.177698  0.010468  0.154517 -0.127518   \n",
              "2     0.160725  0.370406  0.206674  0.177689  0.010630  0.154396 -0.127385   \n",
              "3     0.160830  0.370194  0.206678  0.177680  0.010792  0.154276 -0.127253   \n",
              "4     0.160935  0.369981  0.206683  0.177671  0.010954  0.154155 -0.127121   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995  0.369619 -0.053489  0.215722  0.159651  0.334012 -0.085641  0.136309   \n",
              "1996  0.369723 -0.053702  0.215727  0.159642  0.334175 -0.085762  0.136442   \n",
              "1997  0.369828 -0.053915  0.215732  0.159633  0.334337 -0.085882  0.136574   \n",
              "1998  0.369933 -0.054127  0.215736  0.159624  0.334499 -0.086003  0.136706   \n",
              "1999  0.370038 -0.054340  0.215741  0.159615  0.334661 -0.086123  0.136839   \n",
              "\n",
              "           49   ...       455       456       457       458       459  \\\n",
              "0     0.091875  ... -0.103888  0.030449 -0.072375  0.084060 -0.200314   \n",
              "1     0.091881  ... -0.103818  0.030347 -0.072356  0.084055 -0.200269   \n",
              "2     0.091888  ... -0.103748  0.030245 -0.072337  0.084051 -0.200223   \n",
              "3     0.091894  ... -0.103678  0.030143 -0.072318  0.084047 -0.200178   \n",
              "4     0.091901  ... -0.103608  0.030041 -0.072299  0.084043 -0.200132   \n",
              "...        ...  ...       ...       ...       ...       ...       ...   \n",
              "1995  0.104756  ...  0.035964 -0.173130 -0.034843  0.075910 -0.109507   \n",
              "1996  0.104762  ...  0.036034 -0.173232 -0.034824  0.075906 -0.109461   \n",
              "1997  0.104769  ...  0.036104 -0.173334 -0.034805  0.075902 -0.109416   \n",
              "1998  0.104775  ...  0.036174 -0.173436 -0.034787  0.075898 -0.109370   \n",
              "1999  0.104781  ...  0.036244 -0.173538 -0.034768  0.075894 -0.109325   \n",
              "\n",
              "           460       461       462       463       464       465       466  \\\n",
              "0    -0.173415 -0.078824  0.032272 -0.327215  0.246959  0.092916  0.136684   \n",
              "1    -0.173422 -0.078786  0.032160 -0.327094  0.246769  0.092836  0.136521   \n",
              "2    -0.173429 -0.078748  0.032048 -0.326972  0.246579  0.092756  0.136358   \n",
              "3    -0.173436 -0.078711  0.031936 -0.326851  0.246389  0.092676  0.136194   \n",
              "4    -0.173443 -0.078673  0.031824 -0.326730  0.246200  0.092597  0.136031   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995 -0.187538 -0.003215 -0.190824 -0.085187 -0.131676 -0.066359 -0.188736   \n",
              "1996 -0.187546 -0.003177 -0.190936 -0.085066 -0.131866 -0.066439 -0.188899   \n",
              "1997 -0.187553 -0.003139 -0.191048 -0.084945 -0.132056 -0.066519 -0.189063   \n",
              "1998 -0.187560 -0.003101 -0.191160 -0.084823 -0.132245 -0.066599 -0.189226   \n",
              "1999 -0.187567 -0.003063 -0.191271 -0.084702 -0.132435 -0.066679 -0.189389   \n",
              "\n",
              "           467       468       469       470       471       472       473  \\\n",
              "0     0.052101 -0.117799 -0.097961  0.025057 -0.079914  0.003686  0.017816   \n",
              "1     0.052175 -0.117762 -0.097835  0.024982 -0.079784  0.003654  0.017685   \n",
              "2     0.052249 -0.117725 -0.097708  0.024906 -0.079653  0.003623  0.017555   \n",
              "3     0.052323 -0.117689 -0.097581  0.024831 -0.079523  0.003592  0.017424   \n",
              "4     0.052397 -0.117652 -0.097454  0.024755 -0.079393  0.003561  0.017294   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995  0.199669 -0.044898  0.154852 -0.125500  0.180192 -0.058423 -0.242590   \n",
              "1996  0.199743 -0.044862  0.154979 -0.125575  0.180322 -0.058454 -0.242720   \n",
              "1997  0.199817 -0.044825  0.155105 -0.125651  0.180453 -0.058485 -0.242851   \n",
              "1998  0.199891 -0.044789  0.155232 -0.125726  0.180583 -0.058516 -0.242981   \n",
              "1999  0.199965 -0.044752  0.155359 -0.125802  0.180714 -0.058547 -0.243112   \n",
              "\n",
              "           474       475       476       477       478       479       480  \\\n",
              "0     0.162660  0.151691 -0.169811 -0.286977  0.046300  0.162457 -0.068755   \n",
              "1     0.162680  0.151633 -0.169867 -0.286736  0.046273  0.162390 -0.068749   \n",
              "2     0.162700  0.151575 -0.169924 -0.286495  0.046246  0.162322 -0.068743   \n",
              "3     0.162720  0.151516 -0.169981 -0.286254  0.046220  0.162255 -0.068738   \n",
              "4     0.162740  0.151458 -0.170037 -0.286013  0.046193  0.162188 -0.068732   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995  0.202217  0.035215 -0.282732  0.194125 -0.007162  0.028559 -0.057462   \n",
              "1996  0.202237  0.035157 -0.282788  0.194366 -0.007188  0.028492 -0.057456   \n",
              "1997  0.202256  0.035099 -0.282845  0.194607 -0.007215  0.028424 -0.057450   \n",
              "1998  0.202276  0.035040 -0.282902  0.194849 -0.007242  0.028357 -0.057445   \n",
              "1999  0.202296  0.034982 -0.282958  0.195090 -0.007269  0.028290 -0.057439   \n",
              "\n",
              "           481       482       483       484       485       486       487  \\\n",
              "0    -0.324309  0.198030 -0.074244  0.033139  0.059784 -0.128351  0.217884   \n",
              "1    -0.324112  0.198118 -0.074206  0.033185  0.059790 -0.128185  0.217637   \n",
              "2    -0.323914  0.198207 -0.074169  0.033232  0.059796 -0.128019  0.217390   \n",
              "3    -0.323717  0.198295 -0.074132  0.033278  0.059802 -0.127853  0.217143   \n",
              "4    -0.323519  0.198384 -0.074095  0.033325  0.059809 -0.127687  0.216896   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995  0.069736  0.374694  0.000167  0.125860  0.072188  0.202831 -0.274979   \n",
              "1996  0.069934  0.374783  0.000204  0.125906  0.072194  0.202997 -0.275226   \n",
              "1997  0.070132  0.374871  0.000242  0.125953  0.072200  0.203163 -0.275473   \n",
              "1998  0.070329  0.374960  0.000279  0.125999  0.072207  0.203329 -0.275720   \n",
              "1999  0.070527  0.375048  0.000316  0.126046  0.072213  0.203495 -0.275968   \n",
              "\n",
              "           488       489       490       491       492       493       494  \\\n",
              "0     0.064765 -0.019849 -0.094946  0.025600  0.126472 -0.141238 -0.083810   \n",
              "1     0.064824 -0.019906 -0.094876  0.025602  0.126366 -0.141211 -0.083907   \n",
              "2     0.064883 -0.019963 -0.094807  0.025603  0.126259 -0.141184 -0.084005   \n",
              "3     0.064942 -0.020020 -0.094737  0.025605  0.126153 -0.141157 -0.084102   \n",
              "4     0.065000 -0.020076 -0.094668  0.025606  0.126047 -0.141130 -0.084199   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995  0.181876 -0.133127  0.043893  0.028344 -0.085819 -0.087385 -0.278012   \n",
              "1996  0.181935 -0.133183  0.043962  0.028345 -0.085926 -0.087358 -0.278109   \n",
              "1997  0.181994 -0.133240  0.044032  0.028346 -0.086032 -0.087331 -0.278206   \n",
              "1998  0.182052 -0.133297  0.044101  0.028348 -0.086139 -0.087304 -0.278304   \n",
              "1999  0.182111 -0.133354  0.044171  0.028349 -0.086245 -0.087277 -0.278401   \n",
              "\n",
              "           495       496       497       498       499       500       501  \\\n",
              "0     0.059785 -0.105976  0.078958 -0.133958  0.125615  0.002990 -0.087544   \n",
              "1     0.059759 -0.105940  0.078900 -0.133892  0.125618  0.002928 -0.087578   \n",
              "2     0.059732 -0.105905  0.078842 -0.133826  0.125621  0.002866 -0.087613   \n",
              "3     0.059705 -0.105869  0.078785 -0.133761  0.125624  0.002805 -0.087647   \n",
              "4     0.059679 -0.105834  0.078727 -0.133695  0.125626  0.002743 -0.087681   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995  0.006700 -0.035158 -0.036274 -0.002642  0.131440 -0.119885 -0.156117   \n",
              "1996  0.006674 -0.035123 -0.036332 -0.002576  0.131443 -0.119946 -0.156151   \n",
              "1997  0.006647 -0.035087 -0.036390 -0.002511  0.131445 -0.120008 -0.156185   \n",
              "1998  0.006621 -0.035052 -0.036447 -0.002445  0.131448 -0.120070 -0.156220   \n",
              "1999  0.006594 -0.035016 -0.036505 -0.002379  0.131451 -0.120131 -0.156254   \n",
              "\n",
              "           502       503       504  \n",
              "0    -0.195172 -0.035026  0.087696  \n",
              "1    -0.195038 -0.035052  0.087563  \n",
              "2    -0.194904 -0.035079  0.087431  \n",
              "3    -0.194770 -0.035105  0.087299  \n",
              "4    -0.194636 -0.035132  0.087167  \n",
              "...        ...       ...       ...  \n",
              "1995  0.072136 -0.087926 -0.175866  \n",
              "1996  0.072270 -0.087953 -0.175998  \n",
              "1997  0.072404 -0.087980 -0.176130  \n",
              "1998  0.072538 -0.088006 -0.176262  \n",
              "1999  0.072672 -0.088033 -0.176394  \n",
              "\n",
              "[2000 rows x 505 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2sXVZd2OQAr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a877723b-b31f-47ff-effb-25ec892dacf1"
      },
      "source": [
        "print(pd.DataFrame(X).describe().loc['min'].min())\n",
        "print(pd.DataFrame(X).describe().loc['max'].max())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.5396492997164709\n",
            "0.47779568492737506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eqjq1q-Iszy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "aa39ceb9-48a8-49ea-e96a-053a63f5e6f6"
      },
      "source": [
        "X = np.array(X, dtype=np.float32)\n",
        "X.dtype"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnUuXl1V6OIe",
        "colab_type": "text"
      },
      "source": [
        "#### Build an environment to represent this movement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oeGuXNq6OIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StockMarketEnv(py_environment.PyEnvironment):\n",
        "    '''\n",
        "    Observation: The observation should be a (90,505) matrix\n",
        "    Action: A (505) vector with probabilties from 0 1, max 10 are encoded as 1, all others are 0\n",
        "    Reward: dot product of the (505,1) top 10 choices with the next (1,505) returns\n",
        "    '''\n",
        "    def __init__(self, X):\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "                                    shape=(1020,), dtype=np.float32, minimum=0, maximum=1, name='action')\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "                                    shape=(90,505), dtype=np.float32, minimum=-1, maximum=1 ,name='observation')\n",
        "        self._X = X\n",
        "        self._state = np.array(self._X[:90], dtype=np.float32)\n",
        "        self._i = 0\n",
        "        self._episode_ended = False\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        self._state = np.array(self._X[:90], dtype=np.float32) ## input array\n",
        "        self._i = 0\n",
        "        self._episode_ended = False\n",
        "        return ts.restart(self._state)\n",
        "\n",
        "    def _step(self, action):\n",
        "        '''\n",
        "        Given a state array:\n",
        "            - Choose the top ten stocks\n",
        "            - Compute the reward by taking the dot product\n",
        "            - Update the new state by taking the next timestep\n",
        "            - Return the ts.transition(new_state, reward, discount=1)\n",
        "        '''\n",
        "        if self._episode_ended:\n",
        "            return self.reset()\n",
        "        \n",
        "        action_state = action.copy()\n",
        "        # action_state[action_state.argsort()[-10:]] = 1\n",
        "        \n",
        "        # mask = np.ones(action_state.shape, bool)\n",
        "        # mask[action_state.argsort()[-10:]] = False\n",
        "        # action_state[mask] = 0\n",
        "        \n",
        "        reward_state = np.array(self._X[91+self._i], dtype=np.float64)\n",
        "        \n",
        "        ## modify for hold and sell\n",
        "        # reward = np.dot(reward_state, action_state)[0]\n",
        "        reward_pos = np.dot(reward_state, action_state[:505])\n",
        "        reward_neg = np.dot(-reward_state, action_state[515:])\n",
        "        reward = reward_pos + reward_neg\n",
        "        # 10 possible action states for holding cash, idx [505:515] -> zero reward\n",
        "        \n",
        "        self._i += 1\n",
        "        \n",
        "        if self._i + 91 >= self._X.shape[0]:\n",
        "            self._episode_ended = True\n",
        "        \n",
        "        self._state = np.array(self._X[self._i:90+self._i], dtype=np.float64)\n",
        "        \n",
        "        if self._episode_ended:\n",
        "            return ts.termination(np.array(self._state, dtype=np.float32), reward=np.array(reward, dtype=np.float32))\n",
        "        else:\n",
        "            return ts.transition(np.array(self._state, dtype=np.float32), reward=np.array(reward, dtype=np.float32), discount=1.0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3GEo5XA6OIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "environment = StockMarketEnv(X)\n",
        "utils.validate_py_environment(environment, episodes=5)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiQVPe-w6OIi",
        "colab_type": "text"
      },
      "source": [
        "##### Train a DQN to learn on it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKhQj07I8-MX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Important interval values for training speed\n",
        "# Set lower to increase speed of training \n",
        "\n",
        "num_iterations = 10000 # @param {type:\"integer\"}\n",
        "log_interval = 500 # @param {type:\"integer\"}=\n",
        "num_eval_episodes = 1 # @param {type:\"integer\"}\n",
        "eval_interval = 1000 # @param {type:\"integer\"}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI8CjCKP6OIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
        "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 256 # @param {type:\"integer\"}\n",
        "\n",
        "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "target_update_tau = 0.005 # @param {type:\"number\"}\n",
        "target_update_period = 1 # @param {type:\"number\"}\n",
        "gamma = 0.99 # @param {type:\"number\"}\n",
        "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
        "\n",
        "actor_fc_layer_params = (505, 1020)\n",
        "critic_joint_fc_layer_params = (505, 1020)\n",
        "\n",
        "\n",
        "\n",
        "policy_save_interval = 5000 # @param {type:\"integer\"}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbxnI01V6OIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "8c434f58-7bc1-44bb-f808-3c47daebda9f"
      },
      "source": [
        "X.shape[0], X.shape[0]//2"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C41tw48g6OIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to produce tf environments\n",
        "# train_py_env = StockMarketEnv(X[:X.shape[0]//2])\n",
        "# eval_py_env = StockMarketEnv(X[X.shape[0]//2:])\n",
        "# train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "# eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "\n",
        "# Code to produce py environments \n",
        "train_env = StockMarketEnv(X[:X.shape[0]//2])\n",
        "eval_env = StockMarketEnv(X[X.shape[0]//2:])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCygvxH_6OIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "b4ae1069-6a85-4fbf-d966-4158ef21063a"
      },
      "source": [
        "print('Observation Spec:')\n",
        "print(train_env.time_step_spec().observation)\n",
        "print('Action Spec:')\n",
        "print(train_env.action_spec())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation Spec:\n",
            "BoundedArraySpec(shape=(90, 505), dtype=dtype('float32'), name='observation', minimum=-10000.0, maximum=10000.0)\n",
            "Action Spec:\n",
            "BoundedArraySpec(shape=(1020,), dtype=dtype('float32'), name='action', minimum=0.0, maximum=1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcyFz4H46OIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_gpu = False \n",
        "\n",
        "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26HrgAyp6OIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "observation_spec, action_spec, time_step_spec = (spec_utils.get_tensor_specs(train_env))\n",
        "\n",
        "with strategy.scope():\n",
        "    critic_net = critic_network.CriticNetwork((observation_spec, action_spec),\n",
        "                                            observation_fc_layer_params=None,\n",
        "                                            action_fc_layer_params=None,\n",
        "                                            joint_fc_layer_params=critic_joint_fc_layer_params,\n",
        "                                            kernel_initializer='glorot_uniform',\n",
        "                                            last_kernel_initializer='glorot_uniform')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1BSlzeq6OI0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3822ed32-3692-4925-dfb4-633c36312f43"
      },
      "source": [
        "action_spec"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BoundedTensorSpec(shape=(1020,), dtype=tf.float32, name='action', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iox-BtBq6OI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b399c0bf-318c-4444-9f95-e2ce25572e82"
      },
      "source": [
        "observation_spec"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BoundedTensorSpec(shape=(90, 505), dtype=tf.float32, name='observation', minimum=array(-10000., dtype=float32), maximum=array(10000., dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdFSU2mg6OI4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a9734a84-7052-4018-855b-0df706b483a7"
      },
      "source": [
        "actor_fc_layer_params"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(505, 1020)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9hmG_ih6OI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "    actor_net = actor_distribution_network.ActorDistributionNetwork(observation_spec,\n",
        "                                                                    action_spec,\n",
        "                                                                    fc_layer_params=actor_fc_layer_params,\n",
        "                                                                    continuous_projection_net=(\n",
        "                                              tanh_normal_projection_network.TanhNormalProjectionNetwork))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZNrQfK16OI8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2ff9cfc9-7e61-496e-a26a-cee38cece3c6"
      },
      "source": [
        "time_step_spec"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(90, 505), dtype=tf.float32, name='observation', minimum=array(-10000., dtype=float32), maximum=array(10000., dtype=float32)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGkzecjK6OI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "    train_step = train_utils.create_train_step()\n",
        "\n",
        "    tf_agent = sac_agent.SacAgent(\n",
        "                                time_step_spec,\n",
        "                                action_spec,\n",
        "                                actor_network=actor_net,\n",
        "                                critic_network=critic_net,\n",
        "                                actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
        "                                                                learning_rate=actor_learning_rate),\n",
        "                                critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
        "                                                                learning_rate=critic_learning_rate),\n",
        "                                alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
        "                                                                learning_rate=alpha_learning_rate),\n",
        "                                target_update_tau=target_update_tau,\n",
        "                                target_update_period=target_update_period,\n",
        "                                td_errors_loss_fn=tf.math.squared_difference,\n",
        "                                gamma=gamma,\n",
        "                                reward_scale_factor=reward_scale_factor,\n",
        "                                train_step_counter=train_step)\n",
        "\n",
        "    tf_agent.initialize()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gM1JEUN6OJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table_name = 'uniform_table'\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_capacity,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
        "\n",
        "reverb_server = reverb.Server([table])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-qcDPy36OJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    sequence_length=2,\n",
        "    table_name=table_name,\n",
        "    local_server=reverb_server)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEVBb3Vy6OJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = reverb_replay.as_dataset(\n",
        "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
        "experience_dataset_fn = lambda: dataset"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Nx9XuX6OJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_eval_policy = tf_agent.policy\n",
        "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_eval_policy, use_tf_function=True)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNMwXs166OJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_collect_policy = tf_agent.collect_policy\n",
        "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_collect_policy, use_tf_function=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DVAbbe56OJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  reverb_replay.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2,\n",
        "  stride_length=1)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v74jTE1x6OJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_policy = random_py_policy.RandomPyPolicy(\n",
        "  train_env.time_step_spec(), train_env.action_spec())"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssnyf2ZI6OJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_collect_actor = actor.Actor(\n",
        "              train_env,\n",
        "              random_policy,\n",
        "              train_step,\n",
        "              steps_per_run=initial_collect_steps,\n",
        "              observers=[rb_observer])\n",
        "initial_collect_actor.run()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78nkhZQb6OJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_step_metric = py_metrics.EnvironmentSteps()\n",
        "collect_actor = actor.Actor(\n",
        "                  train_env,\n",
        "                  collect_policy,\n",
        "                  train_step,\n",
        "                  steps_per_run=1,\n",
        "                  metrics=actor.collect_metrics(10),\n",
        "                  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
        "                  observers=[rb_observer, env_step_metric])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yodc_CfJ6OJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_actor = actor.Actor(\n",
        "  eval_env,\n",
        "  eval_policy,\n",
        "  train_step,\n",
        "  episodes_per_run=num_eval_episodes,\n",
        "  metrics=actor.eval_metrics(num_eval_episodes),\n",
        "  summary_dir=os.path.join(tempdir, 'eval'),\n",
        ")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcF7aCzv6OJT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "1c743ee0-1946-4a12-c733-8130b623a6d9"
      },
      "source": [
        "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
        "\n",
        "# Triggers to save the agent's policy checkpoints.\n",
        "learning_triggers = [\n",
        "    triggers.PolicySavedModelTrigger(\n",
        "        saved_model_dir,\n",
        "        tf_agent,\n",
        "        train_step,\n",
        "        interval=policy_save_interval),\n",
        "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
        "]\n",
        "\n",
        "agent_learner = learner.Learner(\n",
        "  tempdir,\n",
        "  train_step,\n",
        "  tf_agent,\n",
        "  experience_dataset_fn,\n",
        "  triggers=learning_triggers)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/linalg/linear_operator_diag.py:166: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Do not pass `graph_parents`.  They will  no longer be used.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7fd7bdcd2b38>\". Calling saved_model.distribution() will raise the following assertion error: Unable to make a CompositeTensor for \"tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)\" of type `<class 'tf_agents.distributions.utils.SquashToSpecNormal'>`. Email `tfprobability@tensorflow.org` or file an issue on github if you would benefit from this working. (Unable to convert dependent entry 'scale' of object 'tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)': Failed to convert object of type <class 'tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag'> to Tensor. Contents: <tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag object at 0x7fd7bdc4c160>. Consider casting elements to a supported type.)\n",
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7fd7bdcd2b38>\". Calling saved_model.distribution() will raise the following assertion error: Unable to make a CompositeTensor for \"tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)\" of type `<class 'tf_agents.distributions.utils.SquashToSpecNormal'>`. Email `tfprobability@tensorflow.org` or file an issue on github if you would benefit from this working. (Unable to convert dependent entry 'scale' of object 'tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)': Failed to convert object of type <class 'tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag'> to Tensor. Contents: <tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag object at 0x7fd7b0095e48>. Consider casting elements to a supported type.)\n",
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7fd7bdcd2b38>\". Calling saved_model.distribution() will raise the following assertion error: Unable to make a CompositeTensor for \"tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)\" of type `<class 'tf_agents.distributions.utils.SquashToSpecNormal'>`. Email `tfprobability@tensorflow.org` or file an issue on github if you would benefit from this working. (Unable to convert dependent entry 'scale' of object 'tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)': Failed to convert object of type <class 'tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag'> to Tensor. Contents: <tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag object at 0x7fd7620bed30>. Consider casting elements to a supported type.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/policies/collect_policy/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/policies/collect_policy/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/policies/greedy_policy/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/policies/greedy_policy/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/policies/policy/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/policies/policy/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p0RFzeQ6OJV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1e2944da-032c-4785-ec33-08c0ed19eb59"
      },
      "source": [
        "%%time\n",
        "def get_eval_metrics():\n",
        "    eval_actor.run()\n",
        "    results = {}\n",
        "    for metric in eval_actor.metrics:\n",
        "        results[metric.name] = metric.result()\n",
        "    return results\n",
        "\n",
        "metrics = get_eval_metrics()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.74 s, sys: 168 ms, total: 1.91 s\n",
            "Wall time: 1.77 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zI3plJX6OJY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "6d0044e5-5040-494d-9d0e-22ada961ba91"
      },
      "source": [
        "%%time\n",
        "def log_eval_metrics(step, metrics):\n",
        "    eval_results = (', ').join(\n",
        "              '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
        "    print('step = {0}: {1}'.format(step, eval_results))\n",
        "\n",
        "log_eval_metrics(0, metrics)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step = 0: AverageReturn = -49.755524, AverageEpisodeLength = 909.000000\n",
            "CPU times: user 73 s, sys: 0 ns, total: 73 s\n",
            "Wall time: 101 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHcfpMpTBROF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f8799f31-a447-49fb-f314-b675077cbe9a"
      },
      "source": [
        "training_params = [num_iterations,\n",
        "                    log_interval,\n",
        "                    num_eval_episodes,\n",
        "                    eval_interval]\n",
        "          \n",
        "print(training_params)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10000, 500, 1, 1000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrUeXAbq6OJa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "outputId": "9a8a37ea-d9ef-4ea3-ba87-b0bf563daa50"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "    %%time\n",
        "except:\n",
        "    pass\n",
        "\n",
        "start = time.time()\n",
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = metrics[\"AverageReturn\"]\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "    # Training.\n",
        "    collect_actor.run()\n",
        "    loss_info = agent_learner.run(iterations=1)\n",
        "\n",
        "    # Evaluating.\n",
        "    step = agent_learner.train_step_numpy\n",
        "\n",
        "    if eval_interval and step % eval_interval == 0:\n",
        "        metrics = get_eval_metrics()\n",
        "        log_eval_metrics(step, metrics)\n",
        "        returns.append(metrics[\"AverageReturn\"])\n",
        "\n",
        "    if log_interval and step % log_interval == 0:\n",
        "        print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
        "        minsec = divmod((time.time() - start), 60)\n",
        "        hourmin = divmod(minsec[0], 60)\n",
        "        print(f\"Time elasped: {int(hourmin[0])}:{int(hourmin[1])}:{minsec[1]}\")\n",
        "\n",
        "rb_observer.close()\n",
        "reverb_server.stop()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 s, sys: 0 ns, total: 2 s\n",
            "Wall time: 4.77 s\n",
            "step = 500: loss = -22.973289489746094\n",
            "Time elasped: 0:2:10.245006561279297\n",
            "step = 1000: AverageReturn = 247.676712, AverageEpisodeLength = 909.000000\n",
            "step = 1000: loss = -77.79386901855469\n",
            "Time elasped: 0:4:24.699620008468628\n",
            "step = 1500: loss = -150.63677978515625\n",
            "Time elasped: 0:6:25.784976720809937\n",
            "step = 2000: AverageReturn = 1263.975464, AverageEpisodeLength = 909.000000\n",
            "step = 2000: loss = -216.87747192382812\n",
            "Time elasped: 0:8:42.66874384880066\n",
            "step = 2500: loss = -282.59234619140625\n",
            "Time elasped: 0:10:42.31461048126221\n",
            "step = 3000: AverageReturn = 2593.271240, AverageEpisodeLength = 909.000000\n",
            "step = 3000: loss = -355.6470947265625\n",
            "Time elasped: 0:12:59.927887201309204\n",
            "step = 3500: loss = -424.946044921875\n",
            "Time elasped: 0:15:0.5767567157745361\n",
            "step = 4000: AverageReturn = 3371.513916, AverageEpisodeLength = 909.000000\n",
            "step = 4000: loss = -493.1053161621094\n",
            "Time elasped: 0:17:17.404439687728882\n",
            "step = 4500: loss = -566.6144409179688\n",
            "Time elasped: 0:19:21.514500617980957\n",
            "step = 5000: AverageReturn = 4105.432617, AverageEpisodeLength = 909.000000\n",
            "step = 5000: loss = -647.98779296875\n",
            "Time elasped: 0:21:35.35584807395935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-97ce1ef3af59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcollect_actor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Evaluating.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/experimental/train/learner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, iterations, iterator)\u001b[0m\n\u001b[1;32m    174\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experience_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       \u001b[0mloss_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       \u001b[0mtrain_step_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W3rf_rGT-BK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5f4bb5e2-86af-4dab-f1b6-6001d70c342c"
      },
      "source": [
        "steps"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 10001, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2yOcUKHT_ka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e5915f06-b43a-4758-d4a8-7ba9da623a07"
      },
      "source": [
        "returns"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-49.755524, 247.67671, 1263.9755, 2593.2712, 3371.514, 4105.4326]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmtrxGbo6OJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7c8c7ab2-587f-4def-d7fa-34fd1cded2ea"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "steps = range(0, 5500 + 1, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-257.51493072509766, 4313.192024230957)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYWwI0o_CD9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "54385f82-c634-47ab-ce92-11b17e78761b"
      },
      "source": [
        "for metric in collect_actor.metrics:\n",
        "  print(f\"{metric.name} = {metric.result()}\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NumberOfEpisodes = 5\n",
            "EnvironmentSteps = 5049\n",
            "AverageReturn = 1078.9906005859375\n",
            "AverageEpisodeLength = 909.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2fSUEvIYKvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "fcc017c2-18d3-45db-f68b-5970b099c9a2"
      },
      "source": [
        "collect_actor.metrics[2].result()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1078.9906"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fDdoelycp4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}