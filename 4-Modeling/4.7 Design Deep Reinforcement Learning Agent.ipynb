{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.7 Design Deep Reinforcement Learning Agent\n",
    "\n",
    "In this notebook we will consider an alternative question. Instead of asking how we can maximize profit on a single asset, we will ask whether a machine can pick the best assets given many selections. \n",
    "\n",
    "We will consider only price history data. We will provide 90 differenced timesteps on the minute interval period. We will choose the 500 stocks of the S&P 500. \n",
    "\n",
    "If our network can outperform the S&P 500 over the given time, we will consider it successful. \n",
    "\n",
    "In order to do this, we will need to perform the following steps:\n",
    "\n",
    "- download datasets for all the stocks in the S&P 500. \n",
    "- format the data to represent the simulataneous movement of 500 stocks\n",
    "- Build an environment to represent this movement\n",
    "- Train a DQN to learn on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download datasets for all the stocks in the S&P 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "from extract import extract_stock, extract_multi_periods, load_set\n",
    "from transform import format_date\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sine_modules import *\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import base64\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.experimental.train import actor\n",
    "from tf_agents.experimental.train import learner\n",
    "from tf_agents.experimental.train import triggers\n",
    "from tf_agents.experimental.train.utils import spec_utils\n",
    "from tf_agents.experimental.train.utils import strategy_utils\n",
    "from tf_agents.experimental.train.utils import train_utils\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "import reverb\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "data_dir = './data/sp500/'\n",
    "suffix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/sp500/spdfm.pickle'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{data_dir}spdfm.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdfm = pd.read_pickle(f'{data_dir}spdfm.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdfm.to_csv(f'{data_dir}spdfm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spdfm.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7792, 505)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build an environment to represent this movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockMarketEnv(py_environment.PyEnvironment):\n",
    "    '''\n",
    "    Observation: The observation should be a (90,505) matrix\n",
    "    Action: A (505) vector with probabilties from 0 1, max 10 are encoded as 1, all others are 0\n",
    "    Reward: dot product of the (505,1) top 10 choices with the next (1,505) returns\n",
    "    '''\n",
    "    def __init__(self, X):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "                                    shape=(1020,1), dtype=np.float32, minimum=0, maximum=1, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "                                    shape=(90,505), dtype=np.float32, minimum=-10, maximum=10 ,name='observation')\n",
    "        self._X = X\n",
    "        self._state = np.array(self._X[:90], dtype=np.float32)\n",
    "        self._i = 0\n",
    "        self._episode_ended = False\n",
    "        \n",
    "#         self._step_type_spec = array_spec.BoundedArraySpec(\n",
    "#                                     shape=(1,), dtype=np.int32, name='step_type')\n",
    "#         self._reward_spec = array_spec.BoundedArraySpec(\n",
    "#                                     shape=(1,), dtype=np.float32, name='reward')\n",
    "#         self._discount_spec = array_spec.BoundedArraySpec(\n",
    "#                                     shape=(1,), dtype=np.float32, name='discount')\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.array(self._X[:90], dtype=np.float32) ## input array\n",
    "        self._i = 0\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(self._state)\n",
    "\n",
    "    def _step(self, action):\n",
    "        '''\n",
    "        Given a state array:\n",
    "            - Choose the top ten stocks\n",
    "            - Compute the reward by taking the dot product\n",
    "            - Update the new state by taking the next timestep\n",
    "            - Return the ts.transition(new_state, reward, discount=1)\n",
    "        '''\n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            return self.reset()\n",
    "        \n",
    "        action_state = action.copy()\n",
    "        action_state[action_state.argsort()[-10:]] = 1\n",
    "        \n",
    "        mask = np.ones(action_state.shape, bool)\n",
    "        mask[action_state.argsort()[-10:]] = False\n",
    "        action_state[mask] = 0\n",
    "        \n",
    "        reward_state = np.array(self._X[91+self._i], dtype=np.float64)\n",
    "        \n",
    "        ## modify for hold and sell\n",
    "        # reward = np.dot(reward_state, action_state)[0]\n",
    "        reward_pos = np.dot(reward_state, action_state[:505])[0]\n",
    "        reward_neg = np.dot(-reward_state, action_state[515:])[0]\n",
    "        reward = reward_pos + reward_neg\n",
    "        # 10 possible action states for holding cash, idx [505:515] -> zero reward\n",
    "        \n",
    "        self._i += 1\n",
    "        \n",
    "        if self._i + 91 >= self._X.shape[0]:\n",
    "            self._episode_ended = True\n",
    "        \n",
    "        self._state = np.array(self._X[self._i:90+self._i], dtype=np.float64)\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            return ts.termination(np.array(self._state, dtype=np.float32), reward=np.array(reward, dtype=np.float32))\n",
    "        else:\n",
    "            return ts.transition(np.array(self._state, dtype=np.float32), reward=np.array(reward, dtype=np.float32), discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = StockMarketEnv(X)\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a DQN to learn on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
    "# 1e5 is just so this doesn't take too long (1 hr)\n",
    "num_iterations = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 256 # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "\n",
    "actor_fc_layer_params = (505, 1020)\n",
    "critic_joint_fc_layer_params = (505, 1020)\n",
    "\n",
    "log_interval = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
    "eval_interval = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "policy_save_interval = 5000 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7792, 3896)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0], X.shape[0]//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to produce tf environments\n",
    "# train_py_env = StockMarketEnv(X[:X.shape[0]//2])\n",
    "# eval_py_env = StockMarketEnv(X[X.shape[0]//2:])\n",
    "# train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "# eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "# Code to produce py environments \n",
    "train_env = StockMarketEnv(X[:X.shape[0]//2])\n",
    "eval_env = StockMarketEnv(X[X.shape[0]//2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(90, 505), dtype=dtype('float32'), name='observation', minimum=-10.0, maximum=10.0)\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(1020, 1), dtype=dtype('float32'), name='action', minimum=0.0, maximum=1.0)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(train_env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "collect_env = suite_pybullet.load(env_name)\n",
    "eval_env = suite_pybullet.load(env_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False \n",
    "\n",
    "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_spec, action_spec, time_step_spec = (spec_utils.get_tensor_specs(train_env))\n",
    "\n",
    "with strategy.scope():\n",
    "    critic_net = critic_network.CriticNetwork((observation_spec, action_spec),\n",
    "                                            observation_fc_layer_params=None,\n",
    "                                            action_fc_layer_params=None,\n",
    "                                            joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "                                            kernel_initializer='glorot_uniform',\n",
    "                                            last_kernel_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(1020, 1), dtype=tf.float32, name='action', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(90, 505), dtype=tf.float32, name='observation', minimum=array(-10., dtype=float32), maximum=array(10., dtype=float32))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(505, 1020)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_fc_layer_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    actor_net = actor_distribution_network.ActorDistributionNetwork(observation_spec,\n",
    "                                                                    action_spec,\n",
    "                                                                    fc_layer_params=actor_fc_layer_params,\n",
    "                                                                    continuous_projection_net=(\n",
    "                                              tanh_normal_projection_network.TanhNormalProjectionNetwork))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(90, 505), dtype=tf.float32, name='observation', minimum=array(-10., dtype=float32), maximum=array(10., dtype=float32)))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    train_step = train_utils.create_train_step()\n",
    "\n",
    "    tf_agent = sac_agent.SacAgent(\n",
    "                                time_step_spec,\n",
    "                                action_spec,\n",
    "                                actor_network=actor_net,\n",
    "                                critic_network=critic_net,\n",
    "                                actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "                                                                learning_rate=actor_learning_rate),\n",
    "                                critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "                                                                learning_rate=critic_learning_rate),\n",
    "                                alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "                                                                learning_rate=alpha_learning_rate),\n",
    "                                target_update_tau=target_update_tau,\n",
    "                                target_update_period=target_update_period,\n",
    "                                td_errors_loss_fn=tf.math.squared_difference,\n",
    "                                gamma=gamma,\n",
    "                                reward_scale_factor=reward_scale_factor,\n",
    "                                train_step_counter=train_step)\n",
    "\n",
    "    tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'uniform_table'\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_capacity,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
    "\n",
    "reverb_server = reverb.Server([table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    sequence_length=2,\n",
    "    table_name=table_name,\n",
    "    local_server=reverb_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = reverb_replay.as_dataset(\n",
    "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
    "experience_dataset_fn = lambda: dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_eval_policy = tf_agent.policy\n",
    "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_eval_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_collect_policy = tf_agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_collect_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  reverb_replay.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2,\n",
    "  stride_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(\n",
    "  train_env.time_step_spec(), train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_collect_actor = actor.Actor(\n",
    "              train_env,\n",
    "              random_policy,\n",
    "              train_step,\n",
    "              steps_per_run=initial_collect_steps,\n",
    "              observers=[rb_observer])\n",
    "initial_collect_actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "collect_actor = actor.Actor(\n",
    "                  train_env,\n",
    "                  collect_policy,\n",
    "                  train_step,\n",
    "                  steps_per_run=1,\n",
    "                  metrics=actor.collect_metrics(10),\n",
    "                  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
    "                  observers=[rb_observer, env_step_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_actor = actor.Actor(\n",
    "  eval_env,\n",
    "  eval_policy,\n",
    "  train_step,\n",
    "  episodes_per_run=num_eval_episodes,\n",
    "  metrics=actor.eval_metrics(num_eval_episodes),\n",
    "  summary_dir=os.path.join(tempdir, 'eval'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/linalg/linear_operator_diag.py:166: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/linalg/linear_operator_diag.py:166: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n",
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7f6cf049d5d0>\". Calling saved_model.distribution() will raise the following assertion error: Unable to make a CompositeTensor for \"tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)\" of type `<class 'tf_agents.distributions.utils.SquashToSpecNormal'>`. Email `tfprobability@tensorflow.org` or file an issue on github if you would benefit from this working. (Unable to convert dependent entry 'scale' of object 'tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)': Failed to convert object of type <class 'tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag'> to Tensor. Contents: <tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag object at 0x7f6cf021b750>. Consider casting elements to a supported type.)\n",
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7f6cf049d5d0>\". Calling saved_model.distribution() will raise the following assertion error: Unable to make a CompositeTensor for \"tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)\" of type `<class 'tf_agents.distributions.utils.SquashToSpecNormal'>`. Email `tfprobability@tensorflow.org` or file an issue on github if you would benefit from this working. (Unable to convert dependent entry 'scale' of object 'tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)': Failed to convert object of type <class 'tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag'> to Tensor. Contents: <tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag object at 0x7f6cf01f9d10>. Consider casting elements to a supported type.)\n",
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7f6cf049d5d0>\". Calling saved_model.distribution() will raise the following assertion error: Unable to make a CompositeTensor for \"tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)\" of type `<class 'tf_agents.distributions.utils.SquashToSpecNormal'>`. Email `tfprobability@tensorflow.org` or file an issue on github if you would benefit from this working. (Unable to convert dependent entry 'scale' of object 'tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)': Failed to convert object of type <class 'tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag'> to Tensor. Contents: <tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag object at 0x7f6cec244a90>. Consider casting elements to a supported type.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/policies/collect_policy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/policies/collect_policy/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/policies/greedy_policy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/policies/greedy_policy/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/policies/policy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/policies/policy/assets\n"
     ]
    }
   ],
   "source": [
    "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "\n",
    "# Triggers to save the agent's policy checkpoints.\n",
    "learning_triggers = [\n",
    "    triggers.PolicySavedModelTrigger(\n",
    "        saved_model_dir,\n",
    "        tf_agent,\n",
    "        train_step,\n",
    "        interval=policy_save_interval),\n",
    "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
    "]\n",
    "\n",
    "agent_learner = learner.Learner(\n",
    "  tempdir,\n",
    "  train_step,\n",
    "  tf_agent,\n",
    "  experience_dataset_fn,\n",
    "  triggers=learning_triggers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_metrics():\n",
    "    eval_actor.run()\n",
    "    results = {}\n",
    "    for metric in eval_actor.metrics:\n",
    "        results[metric.name] = metric.result()\n",
    "    return results\n",
    "\n",
    "metrics = get_eval_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_eval_metrics(step, metrics):\n",
    "    eval_results = (', ').join(\n",
    "              '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
    "    print('step = {0}: {1}'.format(step, eval_results))\n",
    "\n",
    "log_eval_metrics(0, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Training.\n",
    "    collect_actor.run()\n",
    "    loss_info = agent_learner.run(iterations=1)\n",
    "\n",
    "    # Evaluating.\n",
    "    step = agent_learner.train_step_numpy\n",
    "\n",
    "    if eval_interval and step % eval_interval == 0:\n",
    "        metrics = get_eval_metrics()\n",
    "        log_eval_metrics(step, metrics)\n",
    "        returns.append(metrics[\"AverageReturn\"])\n",
    "\n",
    "    if log_interval and step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
    "\n",
    "rb_observer.close()\n",
    "reverb_server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
