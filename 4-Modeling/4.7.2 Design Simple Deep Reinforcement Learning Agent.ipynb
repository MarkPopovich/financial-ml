{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.7 Design Deep Reinforcement Learning Agent\n",
    "\n",
    "In this notebook we will consider an alternative question. Instead of asking how we can maximize profit on a single asset, we will ask whether a machine can pick the best assets given many selections. \n",
    "\n",
    "We will consider only price history data. We will provide 90 differenced timesteps on the minute interval period. We will choose the 500 stocks of the S&P 500. \n",
    "\n",
    "If our network can outperform the S&P 500 over the given time, we will consider it successful. \n",
    "\n",
    "In order to do this, we will need to perform the following steps:\n",
    "\n",
    "- download datasets for all the stocks in the S&P 500. \n",
    "- format the data to represent the simulataneous movement of 500 stocks\n",
    "- Build an environment to represent this movement\n",
    "- Train a DQN to learn on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download datasets for all the stocks in the S&P 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "from extract import extract_stock, extract_multi_periods, load_set\n",
    "from transform import format_date\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sine_modules import *\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import base64\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.experimental.train import actor\n",
    "from tf_agents.experimental.train import learner\n",
    "from tf_agents.experimental.train import triggers\n",
    "from tf_agents.experimental.train.utils import spec_utils\n",
    "from tf_agents.experimental.train.utils import strategy_utils\n",
    "from tf_agents.experimental.train.utils import train_utils\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "import reverb\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "data_dir = './data/sp500/'\n",
    "suffix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/sp500/spdfm.pickle'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{data_dir}spdfm.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdfm = pd.read_pickle(f'{data_dir}spdfm.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdfm.to_csv(f'{data_dir}spdfm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime\n",
       "2020-08-10 13:30:00    0.001384\n",
       "2020-08-10 13:31:00    0.001664\n",
       "2020-08-10 13:32:00   -0.000295\n",
       "2020-08-10 13:33:00    0.000954\n",
       "2020-08-10 13:34:00    0.001535\n",
       "                         ...   \n",
       "2020-09-04 19:55:00   -0.000362\n",
       "2020-09-04 19:56:00   -0.000241\n",
       "2020-09-04 19:57:00   -0.000664\n",
       "2020-09-04 19:58:00   -0.000664\n",
       "2020-09-04 19:59:00    0.000724\n",
       "Name: MMM, Length: 7792, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spdfm['MMM'][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spdfm['MMM'][::-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7792,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(spdfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build an environment to represent this movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockMarketEnv(py_environment.PyEnvironment):\n",
    "    '''\n",
    "    Observation: The observation should be a (90,505) matrix\n",
    "    Action: A (505) vector with probabilties from 0 1, max 10 are encoded as 1, all others are 0\n",
    "    Reward: dot product of the (505,1) top 10 choices with the next (1,505) returns\n",
    "    '''\n",
    "    def __init__(self, X):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "                                    shape=(3,), dtype=np.float32, minimum=0, maximum=1, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "                                    shape=(90,), dtype=np.float32, minimum=-10, maximum=10 ,name='observation')\n",
    "        self._X = X\n",
    "        self._state = np.array(self._X[:90], dtype=np.float32)\n",
    "        self._i = 0\n",
    "        self._episode_ended = False\n",
    "        \n",
    "#         self._step_type_spec = array_spec.BoundedArraySpec(\n",
    "#                                     shape=(1,), dtype=np.int32, name='step_type')\n",
    "#         self._reward_spec = array_spec.BoundedArraySpec(\n",
    "#                                     shape=(1,), dtype=np.float32, name='reward')\n",
    "#         self._discount_spec = array_spec.BoundedArraySpec(\n",
    "#                                     shape=(1,), dtype=np.float32, name='discount')\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.array(self._X[:90], dtype=np.float32) ## input array\n",
    "        self._i = 0\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(self._state)\n",
    "\n",
    "    def _step(self, action):\n",
    "        '''\n",
    "        Given a state array:\n",
    "            - Choose the top ten stocks\n",
    "            - Compute the reward by taking the dot product\n",
    "            - Update the new state by taking the next timestep\n",
    "            - Return the ts.transition(new_state, reward, discount=1)\n",
    "        '''\n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            return self.reset()\n",
    "        \n",
    "        action_state = action.copy()\n",
    "        action_state[action_state.argsort()[-1]] = 1\n",
    "        \n",
    "        mask = np.ones(action_state.shape, bool)\n",
    "        mask[action_state.argsort()[-1]] = False\n",
    "        action_state[mask] = 0\n",
    "        \n",
    "        reward_state = np.array([self._X[91+self._i]], dtype=np.float32)\n",
    "        \n",
    "        ## modify for hold and sell\n",
    "        # reward = np.dot(reward_state, action_state)[0]\n",
    "        reward_pos = np.dot(reward_state, action_state[:1])\n",
    "        reward_neg = np.dot(-reward_state, action_state[2:])\n",
    "        reward = reward_pos + reward_neg\n",
    "        # 10 possible action states for holding cash, idx [505:515] -> zero reward\n",
    "        \n",
    "        self._i += 1\n",
    "        \n",
    "        if self._i + 91 >= self._X.shape[0]:\n",
    "            self._episode_ended = True\n",
    "        \n",
    "        self._state = np.array(self._X[self._i:90+self._i], dtype=np.float32)\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            return ts.termination(np.array(self._state, dtype=np.float32), reward=np.array(reward, dtype=np.float32))\n",
    "        else:\n",
    "            return ts.transition(np.array(self._state, dtype=np.float32), reward=np.array(reward, dtype=np.float32), discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = StockMarketEnv(X)\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.297757863998413"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "utils.validate_py_environment(environment, episodes=5)\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a DQN to learn on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
    "# 1e5 is just so this doesn't take too long (1 hr)\n",
    "num_iterations = 1 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 256 # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "\n",
    "actor_fc_layer_params = (128,)\n",
    "critic_joint_fc_layer_params = (128,)\n",
    "\n",
    "log_interval = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
    "eval_interval = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "policy_save_interval = 5000 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7792, 3896)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0], X.shape[0]//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to produce tf environments\n",
    "# train_py_env = StockMarketEnv(X[:X.shape[0]//2])\n",
    "# eval_py_env = StockMarketEnv(X[X.shape[0]//2:])\n",
    "# train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "# eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "# Code to produce py environments \n",
    "train_env = StockMarketEnv(X[:X.shape[0]//2])\n",
    "eval_env = StockMarketEnv(X[X.shape[0]//2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(90,), dtype=dtype('float32'), name='observation', minimum=-10.0, maximum=10.0)\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(3,), dtype=dtype('float32'), name='action', minimum=0.0, maximum=1.0)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(train_env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "collect_env = suite_pybullet.load(env_name)\n",
    "eval_env = suite_pybullet.load(env_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False \n",
    "\n",
    "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_spec, action_spec, time_step_spec = (spec_utils.get_tensor_specs(train_env))\n",
    "\n",
    "with strategy.scope():\n",
    "    critic_net = critic_network.CriticNetwork((observation_spec, action_spec),\n",
    "                                            observation_fc_layer_params=None,\n",
    "                                            action_fc_layer_params=None,\n",
    "                                            joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "                                            kernel_initializer='glorot_uniform',\n",
    "                                            last_kernel_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(3,), dtype=tf.float32, name='action', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(90,), dtype=tf.float32, name='observation', minimum=array(-10., dtype=float32), maximum=array(10., dtype=float32))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_fc_layer_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    actor_net = actor_distribution_network.ActorDistributionNetwork(observation_spec,\n",
    "                                                                    action_spec,\n",
    "                                                                    fc_layer_params=actor_fc_layer_params,\n",
    "                                                                    continuous_projection_net=(\n",
    "                                              tanh_normal_projection_network.TanhNormalProjectionNetwork))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(90,), dtype=tf.float32, name='observation', minimum=array(-10., dtype=float32), maximum=array(10., dtype=float32)))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    train_step = train_utils.create_train_step()\n",
    "\n",
    "    tf_agent = sac_agent.SacAgent(\n",
    "                                time_step_spec,\n",
    "                                action_spec,\n",
    "                                actor_network=actor_net,\n",
    "                                critic_network=critic_net,\n",
    "                                actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "                                                                learning_rate=actor_learning_rate),\n",
    "                                critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "                                                                learning_rate=critic_learning_rate),\n",
    "                                alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "                                                                learning_rate=alpha_learning_rate),\n",
    "                                target_update_tau=target_update_tau,\n",
    "                                target_update_period=target_update_period,\n",
    "                                td_errors_loss_fn=tf.math.squared_difference,\n",
    "                                gamma=gamma,\n",
    "                                reward_scale_factor=reward_scale_factor,\n",
    "                                train_step_counter=train_step)\n",
    "\n",
    "    tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'uniform_table'\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_capacity,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
    "\n",
    "reverb_server = reverb.Server([table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    sequence_length=2,\n",
    "    table_name=table_name,\n",
    "    local_server=reverb_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = reverb_replay.as_dataset(\n",
    "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
    "experience_dataset_fn = lambda: dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_eval_policy = tf_agent.policy\n",
    "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_eval_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_collect_policy = tf_agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_collect_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  reverb_replay.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2,\n",
    "  stride_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(\n",
    "  train_env.time_step_spec(), train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_collect_actor = actor.Actor(\n",
    "              train_env,\n",
    "              random_policy,\n",
    "              train_step,\n",
    "              steps_per_run=initial_collect_steps,\n",
    "              observers=[rb_observer])\n",
    "initial_collect_actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "collect_actor = actor.Actor(\n",
    "                  train_env,\n",
    "                  collect_policy,\n",
    "                  train_step,\n",
    "                  steps_per_run=1,\n",
    "                  metrics=actor.collect_metrics(10),\n",
    "                  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
    "                  observers=[rb_observer, env_step_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_actor = actor.Actor(\n",
    "  eval_env,\n",
    "  eval_policy,\n",
    "  train_step,\n",
    "  episodes_per_run=num_eval_episodes,\n",
    "  metrics=actor.eval_metrics(num_eval_episodes),\n",
    "  summary_dir=os.path.join(tempdir, 'eval'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7ffa2861ad10>\". Calling saved_model.distribution() will raise the following assertion error: Unable to make a CompositeTensor for \"tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)\" of type `<class 'tf_agents.distributions.utils.SquashToSpecNormal'>`. Email `tfprobability@tensorflow.org` or file an issue on github if you would benefit from this working. (Unable to convert dependent entry 'scale' of object 'tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)': Failed to convert object of type <class 'tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag'> to Tensor. Contents: <tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag object at 0x7ffa289baa10>. Consider casting elements to a supported type.)\n",
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7ffa2861ad10>\". Calling saved_model.distribution() will raise the following assertion error: Unable to make a CompositeTensor for \"tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)\" of type `<class 'tf_agents.distributions.utils.SquashToSpecNormal'>`. Email `tfprobability@tensorflow.org` or file an issue on github if you would benefit from this working. (Unable to convert dependent entry 'scale' of object 'tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)': Failed to convert object of type <class 'tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag'> to Tensor. Contents: <tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag object at 0x7ffa2853f450>. Consider casting elements to a supported type.)\n",
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7ffa2861ad10>\". Calling saved_model.distribution() will raise the following assertion error: Unable to make a CompositeTensor for \"tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)\" of type `<class 'tf_agents.distributions.utils.SquashToSpecNormal'>`. Email `tfprobability@tensorflow.org` or file an issue on github if you would benefit from this working. (Unable to convert dependent entry 'scale' of object 'tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_TanhNormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)': Failed to convert object of type <class 'tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag'> to Tensor. Contents: <tensorflow.python.ops.linalg.linear_operator_diag.LinearOperatorDiag object at 0x7ffa285303d0>. Consider casting elements to a supported type.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/policies/collect_policy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/policies/collect_policy/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/policies/greedy_policy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/policies/greedy_policy/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/policies/policy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/policies/policy/assets\n"
     ]
    }
   ],
   "source": [
    "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "\n",
    "# Triggers to save the agent's policy checkpoints.\n",
    "learning_triggers = [\n",
    "    triggers.PolicySavedModelTrigger(\n",
    "        saved_model_dir,\n",
    "        tf_agent,\n",
    "        train_step,\n",
    "        interval=policy_save_interval),\n",
    "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
    "]\n",
    "\n",
    "agent_learner = learner.Learner(\n",
    "  tempdir,\n",
    "  train_step,\n",
    "  tf_agent,\n",
    "  experience_dataset_fn,\n",
    "  triggers=learning_triggers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 35s, sys: 11.6 s, total: 1min 47s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_eval_metrics():\n",
    "    eval_actor.run()\n",
    "    results = {}\n",
    "    for metric in eval_actor.metrics:\n",
    "        results[metric.name] = metric.result()\n",
    "    return results\n",
    "\n",
    "metrics = get_eval_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0: AverageReturn = 0.006437, AverageEpisodeLength = 3805.000000\n"
     ]
    }
   ],
   "source": [
    "def log_eval_metrics(step, metrics):\n",
    "    eval_results = (', ').join(\n",
    "              '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
    "    print('step = {0}: {1}'.format(step, eval_results))\n",
    "\n",
    "log_eval_metrics(0, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82465\n",
      "4975.252615451813\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-5faa77179d99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mcollect_actor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mloss_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tf_agents/experimental/train/actor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     self._time_step, self._policy_state = self._driver.run(\n\u001b[0;32m--> 138\u001b[0;31m         self._time_step, self._policy_state)\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     if (self._write_summaries and self._summary_interval > 0 and\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tf_agents/drivers/py_driver.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mnum_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_steps\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum_episodes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mnext_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tf_agents/policies/py_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m \u001b[0mside\u001b[0m \u001b[0minformation\u001b[0m \u001b[0msuch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0maction\u001b[0m \u001b[0mlog\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tf_agents/policies/py_tf_eager_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# Avoid passing numpy arrays to avoid retracing of the tf.function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mpolicy_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy_action_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     return policy_step._replace(\n\u001b[1;32m     75\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnest_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbatch_nested_tensors_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    531\u001b[0m           (len(args), len(list(self.signature.input_arg))))\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m     \u001b[0mfunction_call_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_call_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfunction_call_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_proto_serialized\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_disabled_rewriter_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mfunction_call_options\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     \"\"\"\n\u001b[1;32m   1064\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_call_options\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m       \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m       \u001b[0;31m# Default to soft placement for functions unless specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mconfig\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;31m# Compute device counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CPU\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GPU\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_physical_devices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "start = time.time()\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    display.clear_output()\n",
    "    print(_)\n",
    "    print(time.time() - start)\n",
    "    # Training.\n",
    "    collect_actor.run()\n",
    "    loss_info = agent_learner.run(iterations=1)\n",
    "\n",
    "    # Evaluating.\n",
    "    step = agent_learner.train_step_numpy\n",
    "\n",
    "    if eval_interval and step % eval_interval == 0:\n",
    "        metrics = get_eval_metrics()\n",
    "        log_eval_metrics(step, metrics)\n",
    "        returns.append(metrics[\"AverageReturn\"])\n",
    "\n",
    "    if log_interval and step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
    "\n",
    "# rb_observer.close()\n",
    "# reverb_server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.011627938924357295, 0.011025489633902907)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.015218849]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
